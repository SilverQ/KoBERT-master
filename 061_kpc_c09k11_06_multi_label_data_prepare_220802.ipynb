{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8d71b8",
   "metadata": {},
   "source": [
    "### KPC C09K11/06\n",
    "* KPC 설계 검증용 데이터를 출원일자 기준으로 분할하여, multi-label 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1eddf56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.38 µs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5b7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from kobert import get_tokenizer\n",
    "from kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f723d24",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3ab2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일의 모든 시트명 읽어와 필요한 시트만 다시 읽기\n",
    "data = pd.read_excel('kpc_c09k/KPC_C09K11_재분류(수정분류표기반).xlsx', sheet_name=None)\n",
    "sheets_list = list(data.keys())[1:]\n",
    "data = pd.read_excel('kpc_c09k/KPC_C09K11_재분류(수정분류표기반).xlsx', sheet_name=sheets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e475d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 dataframe으로 합치기\n",
    "data_total = pd.DataFrame()\n",
    "count = {}\n",
    "for nm in sheets_list:\n",
    "    temp = data[nm]\n",
    "    count[nm] = len(temp)\n",
    "    temp['label'] = nm\n",
    "    if len(data_total) > 0:\n",
    "        data_total = pd.concat([data_total, temp])\n",
    "    else:\n",
    "        data_total = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6603d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1369"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "sum(count.values())  # 1369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139ebd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['연번', '출원번호', '출원일', '공개번호/\\n공고번호', '문서\\n종류', '공개일/\\n공고일', '제목', '출원인',\n",
       "       'CPC 코드', 'KPC 코드', 'label', '비  고', 'Unnamed: 11'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_total.columns\n",
    "# Index(['연번', '출원번호', '출원일', '공개번호/\\n공고번호', '문서\\n종류', '공개일/\\n공고일', '제목', '출원인',\n",
    "# 'CPC 코드', 'KPC 코드', 'label', '비  고', 'Unnamed: 11'], dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c7fd0",
   "metadata": {},
   "source": [
    "#### test 데이터 구분\n",
    "* 출원일이 20200400 이후의 출원번호를 테스트, 이전은 학습 데이터로 사용 (공개/등록건이 일괄 구분되도록)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_date = 20200400\n",
    "test_data = data_total.loc[data_total['출원일'] >= test_split_date]\n",
    "test_count = test_data.groupby(['label'])['출원번호'].count()\n",
    "test_count = dict(test_count)\n",
    "# print(sum(test_count.values()), test_count)  # 473 \n",
    "share = {}\n",
    "for label in count.keys():\n",
    "    if label in test_count.keys():\n",
    "        share[label] = test_count[label] / count[label]\n",
    "    else:\n",
    "        share[label] = 0\n",
    "# share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceba718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공개일로 구분 테스트했던 코드(삭제 예정)\n",
    "# test_data = data_total.loc[data_total['공개일/\\n공고일'] >= 20210600]\n",
    "# test_count = test_data.groupby(['label'])['출원번호'].count()\n",
    "# test_count = dict(test_count)\n",
    "# print(sum(test_count.values()), test_count)  # 395 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182a683",
   "metadata": {},
   "source": [
    "#### 출원번호의 중복을 제거하여 서지사항 검색용 검색식 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d37a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출원번호의 중복을 제거하고 검색식 만들기\n",
    "len(data_total)  # 1369\n",
    "appln_no_uniq = data_total['출원번호'].unique()\n",
    "len(appln_no_uniq)  # 665\n",
    "# print(', '.join(appln_no_uniq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf35307",
   "metadata": {},
   "source": [
    "#### 출원번호 - 라벨 쌍 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = data_total[['출원번호', '출원일', 'label']]\n",
    "label_data.columns = ['appln_no', 'ad', 'label']\n",
    "label_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a97e8",
   "metadata": {},
   "source": [
    "#### 검색결과 서지사항에서 텍스트 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f870080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio = pd.read_excel('kpc_c09k/c09k_861_biblio.xlsx')\n",
    "# biblio[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da9856b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "biblio.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['발명의명칭', '요약', '대표청구항', '과제', '해결방안']\n",
    "replace_strings = ['\\t', '@', '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in text_columns:\n",
    "    for str_ in replace_strings:\n",
    "        biblio[col] = biblio[col].apply(lambda x: x.replace(str_, \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330541f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio['tl_ab'] = biblio[['발명의명칭', '요약']].apply(lambda x: ' '.join(x), axis=1)\n",
    "biblio['tl_cl'] = biblio[['발명의명칭', '대표청구항']].apply(lambda x: ' '.join(x), axis=1)\n",
    "biblio['tl_pu'] = biblio[['발명의명칭', '과제']].apply(lambda x: ' '.join(x), axis=1)\n",
    "biblio['tl_so'] = biblio[['발명의명칭', '해결방안']].apply(lambda x: ' '.join(x), axis=1)\n",
    "text_columns.extend(['tl_ab', 'tl_cl', 'tl_pu', 'tl_so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblio.loc[biblio['대표청구항'].str.len() < 10]  # 대표청구항이 삭제된건 : 총 10건\n",
    "# # 출원번호 : KR102020000043867, KR102020000004267, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c944b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10글자 이하의 텍스트는 학습에서 제외\n",
    "# applnno_text = pd.DataFrame(columns=['appln_no', 'text'])\n",
    "\n",
    "# for cols in text_columns:\n",
    "#     tmp = biblio[['출원번호', cols]]\n",
    "#     tmp.columns = ['appln_no', 'text']\n",
    "#     applnno_text = pd.concat([applnno_text, tmp], axis=0, ignore_index=True)\n",
    "\n",
    "# applnno_text[:2]  # 7749 rows × 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62020b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10글자 이하의 텍스트는 학습에서 제외\n",
    "applnno_text = pd.DataFrame(columns=['appln_no', 'text'])\n",
    "\n",
    "for cols in text_columns:\n",
    "    tmp = biblio[['출원번호', cols]].loc[biblio[cols].str.len() > 10]\n",
    "    tmp.columns = ['appln_no', 'text']\n",
    "    applnno_text = pd.concat([applnno_text, tmp], axis=0, ignore_index=True)\n",
    "\n",
    "applnno_text[:2]  # 7676 rows × 2 columns\n",
    "# text_abst = biblio[['출원번호', '요약']]\n",
    "# text_cl = biblio[['출원번호', '대표청구항']]\n",
    "# text_purpose = biblio[['출원번호', '과제']]\n",
    "# text_solution = biblio[['출원번호', '해결방안']]\n",
    "# text_title.columns = ['appln_no', 'text']\n",
    "# text_abst.columns = ['appln_no', 'text']\n",
    "# text_cl.columns = ['appln_no', 'text']\n",
    "# text_purpose.columns = ['appln_no', 'text']\n",
    "# text_solution.columns = ['appln_no', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea30e67",
   "metadata": {},
   "source": [
    "#### 라벨과 텍스트 데이터 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f259ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_text = pd.DataFrame(columns=['text', 'label'])\n",
    "# label_text\n",
    "# def extract_label_text(text_df, label_df):\n",
    "#     temp = pd.merge(left=label_data, right=text_title, left_index=False, right_index=False, how='inner', on='appln_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb62615",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text = pd.merge(left=label_data, right=applnno_text, left_index=False, right_index=False, how='inner', on='appln_no')\n",
    "label_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b198a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpc_label = label_text['label'].unique()\n",
    "kpc_index_dict = {i: label for i, label in enumerate(kpc_label)}\n",
    "kpc_label_dict = {kpc_index_dict[key]: key for key in kpc_index_dict.keys()}\n",
    "with open('kpc_c09k/c09k_ind_label.pickle', 'wb') as f:\n",
    "    pickle.dump(kpc_index_dict, f)\n",
    "with open('kpc_c09k/c09k_label_ind.pickle', 'wb') as f:\n",
    "    pickle.dump(kpc_label_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text1 = label_text.copy()\n",
    "label_text1['label'] = label_text1['label'].apply(lambda x: kpc_label_dict[x])\n",
    "label_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = label_text1[['text', 'label']].loc[label_text['ad'] < test_split_date]\n",
    "test_text = label_text1[['text', 'label']].loc[label_text['ad'] >= test_split_date]\n",
    "print(len(label_text), len(train_text), len(test_text))  # 15084 9881 5203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746c3ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text.to_csv('kpc_c09k/train_C09K11_220715.txt', encoding='utf-8', sep='\\t', index=False)\n",
    "test_text.to_csv('kpc_c09k/test_C09K11_220715.txt', encoding='utf-8', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a873146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min(train_text['text'].str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c9252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b1446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6414e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50abd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bfcc584",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")  ## CPU\n",
    "device = torch.device(\"cuda:0\")  ## GPU\n",
    "model_path = 'ksic_model'\n",
    "model_path1 = 'c09k_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9671ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size=768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(),\n",
    "                              attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e114b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertmodel = torch.load(os.path.join(model_path, 'KSIC_KoBERT.pt'))  # 전체 모델을 통째로 불러옴, 클래스 선언 필수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c68120",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertmodel.load_state_dict(\n",
    "    torch.load(os.path.join(model_path, 'KSIC_model_state_dict.pt')))  # state_dict를 불러 온 후, 모델에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb61014",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f24348",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bertmodel = torch.load(os.path.join(model_path, 'KSIC_KoBERT.pt'))  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "    bertmodel.load_state_dict(\n",
    "        torch.load(os.path.join(model_path, 'KSIC_model_state_dict.pt')))  # state_dict를 불러 온 후, 모델에 저장\n",
    "    print('Using saved model')\n",
    "    _, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n",
    "    print('complete loading vocab')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90eda0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac104a4",
   "metadata": {},
   "source": [
    "### GluonNLP Toolkit\n",
    "* GluonNLP Toolkit provides tools for building efficient data pipelines for NLP tasks.\n",
    "* https://nlp.gluon.ai/api/modules/data.html#gluonnlp.data.BERTSentenceTransform\n",
    "* class gluonnlp.data.BERTSPTokenizer(path, vocab, num_best=0, alpha=1.0, lower=True, max_input_chars_per_word=200)[source]¶\n",
    "* https://nlp.gluon.ai/api/modules/data.html#gluonnlp.data.TSVDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\".cache/train_H04W4_220511.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\".cache/test_H04W4_220511.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "# 텍스트 내에 줄바꿈과 탭이 존재하여 오류 발생, 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb48a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba861e5",
   "metadata": {},
   "source": [
    "### GluonNLP BERTSPTokenizer\n",
    "* BERTSPTokenizer depends on the sentencepiece library.\n",
    "* For multi-processing with BERTSPTokenizer, making an extra copy of the BERTSPTokenizer instance is recommended before using it.\n",
    "* https://nlp.gluon.ai/api/data.html?highlight=bertsptokenizer#gluonnlp.data.BERTSPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df56e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01745787",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = dataset_train[2][0][:256]\n",
    "# sample_txt = 'GaN 전력 증폭기를 이용한 전력 오실레이터, 본 발명은 GaN(Gallium Nitride) 소자로 구성되며, 입력 신호의 전력을 증폭시켜 출력하는 GaN 전력 증폭기, 상기 GaN 전력 증폭기의 출력 신호의 일부를 피드백 신호로 제공하는 디렉셔널 커플러, 상기 디렉셔널 커플러에 의해서 제공되는 피드백 신호의 페이저를 가변시키는 페이저 시프터 및 상기 페이저 시프터에 의한 임피던스 부정합을 조정하며, 상기 GaN 전력 증폭기로 상기 피드백 신호를 전달하는 제 1 아이솔레이터를 포함하는 GaN 전력 증폭기를 이용한 전력 오실레이터가 제공된다.'\n",
    "print(tok(sample_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train[162][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13899827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "#         self.labels = [i[label_idx] for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90752b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e944083c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 512\n",
    "batch_size = 8\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 1000\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f23e2",
   "metadata": {},
   "source": [
    "#### max_len vs batch size\n",
    "* Text examples should be mostly less than 512 tokens. Longer texts will be cut from the end to fit the sequence length specified in the model block.\n",
    "* https://peltarion.com/knowledge-center/documentation/cheat-sheets/bert---text-classification-/-cheat-sheet\n",
    "* Sequence length\tRecommended max batch size\n",
    "    * 64 - 64, 128 - 32, 256 - 16, 320 - 14, 384 - 12, 512 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_id = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test_id = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aee963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data_train_id.sentences[162])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ab828",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train_id.labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88dc73d",
   "metadata": {},
   "source": [
    "* DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False)\n",
    "* https://pytorch.org/docs/stable/data.html\n",
    "* WARNING\n",
    "    * After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is number of workers * size of parent process). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out issue #13246 for more details on why this occurs and example code for how to workaround these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train_id, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test_id, batch_size=batch_size, num_workers=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, num_classes=5, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895ab36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8e17d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# help(nn.CrossEntropyLoss)\n",
    "# help(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f687765",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506ec8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y')) # ' 1:36PM EDT on Oct 18, 2010'\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y')) # ' 1:36PM EDT on Oct 18, 2010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader2 = torch.utils.data.DataLoader(data_test_id, batch_size=batch_size, num_workers=5)\n",
    "test_acc = 0.0\n",
    "results = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader2), total=len(test_dataloader2)):\n",
    "    print(batch_id)\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    max_vals, max_indices = torch.max(out, 1)\n",
    "    results.extend(max_indices.tolist())\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "    print(\"test acc {}\".format(test_acc / (batch_id+1)))\n",
    "print(results)\n",
    "#     print('label: ', label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d528d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_indices)\n",
    "print(max_indices.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe18709",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "exp = m(out)\n",
    "max_vals, max_indices = torch.max(out, 1)\n",
    "print(max_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae15239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dacon.io/en/competitions/official/235747/codeshare/3082?page=1&dtype=recent\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef28b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5. 모델 학습 과정 표시하기\n",
    "\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(loss.long().to(device), 'y', label='train loss')\n",
    "# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(train_acc.long().to(device), 'b', label='train_acc')\n",
    "acc_ax.plot(test_acc.long().to(device), 'g', label='test_acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(token_ids, valid_length, segment_ids)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ee6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.82it/s]\n",
    "# epoch 1 batch id 1 loss 0.7244999408721924 train acc 0.453125\n",
    "# epoch 1 batch id 201 loss 0.4514557123184204 train acc 0.5766480099502488\n",
    "# epoch 1 batch id 401 loss 0.44362887740135193 train acc 0.6851231296758105\n",
    "# epoch 1 batch id 601 loss 0.5137903094291687 train acc 0.734426996672213\n",
    "# epoch 1 batch id 801 loss 0.43659207224845886 train acc 0.7636352996254682\n",
    "# epoch 1 batch id 1001 loss 0.3066664934158325 train acc 0.7802978271728271\n",
    "# epoch 1 batch id 1201 loss 0.3117211163043976 train acc 0.7930890924229809\n",
    "# epoch 1 batch id 1401 loss 0.30558276176452637 train acc 0.8026075124910778\n",
    "# epoch 1 batch id 1601 loss 0.3304724097251892 train acc 0.8104992973141787\n",
    "# epoch 1 batch id 1801 loss 0.25170841813087463 train acc 0.8164995141588006\n",
    "# epoch 1 batch id 2001 loss 0.2823370695114136 train acc 0.8224481509245377\n",
    "# epoch 1 batch id 2201 loss 0.3238280713558197 train acc 0.8274292935029532\n",
    "# epoch 1 train acc 0.8307713843856656\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 13.16it/s]\n",
    "# epoch 1 test acc 0.8835118286445013\n",
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.81it/s]\n",
    "# epoch 2 batch id 1 loss 0.4955632984638214 train acc 0.828125\n",
    "# epoch 2 batch id 201 loss 0.2225867211818695 train acc 0.8816075870646766\n",
    "# epoch 2 batch id 401 loss 0.33707231283187866 train acc 0.8836502493765586\n",
    "# epoch 2 batch id 601 loss 0.39554905891418457 train acc 0.887115224625624\n",
    "# epoch 2 batch id 801 loss 0.30988579988479614 train acc 0.8883426966292135\n",
    "# epoch 2 batch id 1001 loss 0.28933045268058777 train acc 0.8911713286713286\n",
    "# epoch 2 batch id 1201 loss 0.24474024772644043 train acc 0.8932269983347211\n",
    "# epoch 2 batch id 1401 loss 0.1908964067697525 train acc 0.8957218058529621\n",
    "# epoch 2 batch id 1601 loss 0.23474052548408508 train acc 0.89772993441599\n",
    "# epoch 2 batch id 1801 loss 0.1599130779504776 train acc 0.8997171710161022\n",
    "# epoch 2 batch id 2001 loss 0.20312610268592834 train acc 0.9019865067466267\n",
    "# epoch 2 batch id 2201 loss 0.2386036366224289 train acc 0.9033251930940481\n",
    "# epoch 2 train acc 0.904759047923777\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.77it/s]\n",
    "# epoch 2 test acc 0.8906449808184144\n",
    "# 100%\n",
    "# 2344/2344 [10:51<00:00, 3.81it/s]\n",
    "# epoch 3 batch id 1 loss 0.3390277922153473 train acc 0.875\n",
    "# epoch 3 batch id 201 loss 0.15983553230762482 train acc 0.9240516169154229\n",
    "# epoch 3 batch id 401 loss 0.14856880903244019 train acc 0.9265118453865336\n",
    "# epoch 3 batch id 601 loss 0.2642267644405365 train acc 0.9280366056572379\n",
    "# epoch 3 batch id 801 loss 0.1951659917831421 train acc 0.93020443196005\n",
    "# epoch 3 batch id 1001 loss 0.26453569531440735 train acc 0.9319586663336663\n",
    "# epoch 3 batch id 1201 loss 0.1282612681388855 train acc 0.9340523522064946\n",
    "# epoch 3 batch id 1401 loss 0.1837957799434662 train acc 0.9360501427551748\n",
    "# epoch 3 batch id 1601 loss 0.14137345552444458 train acc 0.9374414428482198\n",
    "# epoch 3 batch id 1801 loss 0.09849003702402115 train acc 0.9387493059411438\n",
    "# epoch 3 batch id 2001 loss 0.15634101629257202 train acc 0.9401080709645178\n",
    "# epoch 3 batch id 2201 loss 0.1982114464044571 train acc 0.9410495229441163\n",
    "# epoch 3 train acc 0.9420039640216155\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.77it/s]\n",
    "# epoch 3 test acc 0.8969988810741688\n",
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.82it/s]\n",
    "# epoch 4 batch id 1 loss 0.388761043548584 train acc 0.875\n",
    "# epoch 4 batch id 201 loss 0.06205718219280243 train acc 0.9593439054726368\n",
    "# epoch 4 batch id 401 loss 0.06854245811700821 train acc 0.9596711346633416\n",
    "# epoch 4 batch id 601 loss 0.23485814034938812 train acc 0.9600925540765392\n",
    "# epoch 4 batch id 801 loss 0.1342923790216446 train acc 0.9612008426966292\n",
    "# epoch 4 batch id 1001 loss 0.1908232569694519 train acc 0.9621472277722277\n",
    "# epoch 4 batch id 1201 loss 0.11091630905866623 train acc 0.9632597835137385\n",
    "# epoch 4 batch id 1401 loss 0.10650145262479782 train acc 0.9642219842969307\n",
    "# epoch 4 batch id 1601 loss 0.08601253479719162 train acc 0.9649242660836976\n",
    "# epoch 4 batch id 1801 loss 0.057230446487665176 train acc 0.9656093836757357\n",
    "# epoch 4 batch id 2001 loss 0.07411567866802216 train acc 0.9665011244377811\n",
    "# epoch 4 batch id 2201 loss 0.1597125381231308 train acc 0.9671456156292594\n",
    "# epoch 4 train acc 0.9676701151877133\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.77it/s]\n",
    "# epoch 4 test acc 0.8980778452685422\n",
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.81it/s]\n",
    "# epoch 5 batch id 1 loss 0.3727969229221344 train acc 0.890625\n",
    "# epoch 5 batch id 201 loss 0.02794063650071621 train acc 0.9752798507462687\n",
    "# epoch 5 batch id 401 loss 0.024620698764920235 train acc 0.9767378428927681\n",
    "# epoch 5 batch id 601 loss 0.15002880990505219 train acc 0.9765495008319468\n",
    "# epoch 5 batch id 801 loss 0.05448848009109497 train acc 0.9766112671660424\n",
    "# epoch 5 batch id 1001 loss 0.08006531745195389 train acc 0.9770541958041958\n",
    "# epoch 5 batch id 1201 loss 0.04451199620962143 train acc 0.9775317443796836\n",
    "# epoch 5 batch id 1401 loss 0.08561042696237564 train acc 0.9780625446109922\n",
    "# epoch 5 batch id 1601 loss 0.026716381311416626 train acc 0.9783533728919426\n",
    "# epoch 5 batch id 1801 loss 0.02442212402820587 train acc 0.9787357717934481\n",
    "# epoch 5 batch id 2001 loss 0.0197431817650795 train acc 0.9790339205397302\n",
    "# epoch 5 batch id 2201 loss 0.03802771866321564 train acc 0.9792565879145843\n",
    "# epoch 5 train acc 0.9794199729806597\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.78it/s]\n",
    "# epoch 5 test acc 0.8974384590792839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea30fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoBERT",
   "language": "python",
   "name": "kobert-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
