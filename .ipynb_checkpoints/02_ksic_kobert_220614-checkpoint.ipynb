{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1eddf56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T06:43:55.353595Z",
     "start_time": "2022-11-24T06:43:55.348298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 10.7 µs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5b7929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T06:43:57.933639Z",
     "start_time": "2022-11-24T06:43:55.860484Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kobert import get_tokenizer\n",
    "from kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW, BertModel\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3ab2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T06:43:57.936950Z",
     "start_time": "2022-11-24T06:43:57.935079Z"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "model_path = 'ksic_model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6be2d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/hdh/PycharmProjects/KoBERT-master/.cache/kobert_v1.zip\n",
      "using cached model. /home/hdh/PycharmProjects/KoBERT-master/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # bertmodel = BertModel.from_pretrained(model_path, return_dict=False)\n",
    "    bertmodel = torch.load(os.path.join(model_path, 'KSIC_KoBERT.pt'))  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "    bertmodel.load_state_dict(\n",
    "        torch.load(os.path.join(model_path, 'KSIC_model_state_dict.pt')))  # state_dict를 불러 온 후, 모델에 저장\n",
    "    _, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n",
    "    print('Using saved model')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90eda0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1488b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_dataset():\n",
    "    try:\n",
    "        train_ds = nlp.data.TSVDataset('.cache/train_ds.tsv', encoding='utf-8',\n",
    "                                       field_indices=[0, 1], num_discard_samples=1)\n",
    "        val_ds = nlp.data.TSVDataset('.cache/val_ds.tsv', encoding='utf-8',\n",
    "                                     field_indices=[0, 1], num_discard_samples=1)\n",
    "        test_ds = nlp.data.TSVDataset('.cache/test_ds.tsv', encoding='utf-8',\n",
    "                                      field_indices=[0, 1], num_discard_samples=1)\n",
    "        print(time.strftime('%l:%M%p %Z on %b %d, %Y'))  # ' 1:36PM EDT on Oct 18, 2010'\n",
    "        print('Loading saved text-label pair dataset completed')\n",
    "        with open('.cache/label_ksic.pickle', 'rb') as f:\n",
    "            ksic_index_dict = pickle.load(f)\n",
    "        with open('.cache/ksic_label.pickle', 'rb') as f:\n",
    "            ksic_label_dict = pickle.load(f)\n",
    "    except:\n",
    "        def load_datasets():\n",
    "            try:\n",
    "                with open('.cache/label_ksic.pickle', 'rb') as f:\n",
    "                    ksic_index_dict = pickle.load(f)\n",
    "                with open('.cache/ksic_label.pickle', 'rb') as f:\n",
    "                    ksic_label_dict = pickle.load(f)\n",
    "                train_input = pd.read_csv('.cache/train_input.csv', encoding='utf-8', low_memory=False)\n",
    "                val_input = pd.read_csv('.cache/val_input.csv', encoding='utf-8', low_memory=False)\n",
    "                test_input = pd.read_csv('.cache/test_input.csv', encoding='utf-8', low_memory=False)\n",
    "                print(time.strftime('%l:%M%p %Z on %b %d, %Y'))  # ' 1:36PM EDT on Oct 18, 2010'\n",
    "                print('Loading saved train_input completed')\n",
    "            except:\n",
    "                fname = \".cache/ksic00.json\"\n",
    "                f_list = [\".cache/ksic00.json\", \".cache/ksic01.json\", \".cache/ksic02.json\"]\n",
    "\n",
    "                with open(fname, encoding='utf-8') as f:\n",
    "                    for line in tqdm(f):\n",
    "                        try:\n",
    "                            # print('line: ', line)\n",
    "                            temp = json.loads(line)\n",
    "                            print(temp['an'])\n",
    "                        #             text = re.sub('[-=.#/?:$}(){,]', ' ', patent['title'] + patent['ab'] + patent['cl'])\n",
    "                        #             token = text.split()\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                def read_column_names(fn):\n",
    "                    with open(fn, encoding='utf-8') as json_f:\n",
    "                        json_line = json_f.readline()\n",
    "                    temp = json.loads(json_line)\n",
    "                    return temp.keys()\n",
    "\n",
    "                col_name = read_column_names(f_list[0])\n",
    "\n",
    "                temp = []\n",
    "                error = []\n",
    "                for fn in f_list[1:]:\n",
    "                    with open(fn, encoding='utf-8') as f:\n",
    "                        for i, line in enumerate(f):\n",
    "                            try:\n",
    "                                temp.append(json.loads(line.replace('\\\\\\\\\"', '\\\\\"')))\n",
    "                            except Exception as e:\n",
    "                                error.append([e, line])\n",
    "                raw_df = pd.DataFrame(data=temp, columns=col_name)\n",
    "\n",
    "                class_count = raw_df['ksic'].value_counts()\n",
    "                class_count2 = class_count[class_count >= 500]\n",
    "\n",
    "                raw_df2 = raw_df.loc[raw_df['ksic'].isin(class_count2.keys())].copy()\n",
    "\n",
    "                ksic_label = raw_df2['ksic'].unique()\n",
    "                ksic_index_dict = {i: label for i, label in enumerate(ksic_label)}\n",
    "                ksic_label_dict = {ksic_index_dict[key]: key for key in ksic_index_dict.keys()}\n",
    "\n",
    "                # ksic_index_dict\n",
    "                with open('.cache/label_ksic.pickle', 'wb') as f:\n",
    "                    pickle.dump(ksic_index_dict, f)\n",
    "                with open('.cache/ksic_label.pickle', 'wb') as f:\n",
    "                    pickle.dump(ksic_label_dict, f)\n",
    "\n",
    "                raw_df2['label'] = raw_df2['ksic'].map(ksic_label_dict)\n",
    "                train_input, test_input = train_test_split(raw_df2, random_state=15, test_size=0.2,\n",
    "                                                           stratify=raw_df2['ksic'],\n",
    "                                                           shuffle=True)\n",
    "                train_input, val_input = train_test_split(train_input, random_state=15, test_size=0.15,\n",
    "                                                          stratify=train_input['ksic'], shuffle=True)\n",
    "\n",
    "                train_input.to_csv('.cache/train_input.csv', encoding='utf-8', mode='w', index=False)\n",
    "                val_input.to_csv('.cache/val_input.csv', encoding='utf-8', mode='w', index=False)\n",
    "                test_input.to_csv('.cache/test_input.csv', encoding='utf-8', mode='w', index=False)\n",
    "                print(time.strftime('%l:%M%p %Z on %b %d, %Y'))  # ' 1:36PM EDT on Oct 18, 2010'\n",
    "                print('Loading json files and saving \"train_input.csv\" completed')\n",
    "            return ksic_index_dict, ksic_label_dict, train_input, val_input, test_input\n",
    "\n",
    "        ksic_index_dict, ksic_label_dict, train_input, val_input, test_input = load_datasets()\n",
    "\n",
    "        def make_input_text(df):\n",
    "            input_tl = df[['title', 'label']].copy()\n",
    "            input_tl.rename(columns={'title': 'text'}, inplace=True)\n",
    "            input_ab = df[['ab', 'label']].copy()\n",
    "            input_ab.rename(columns={'ab': 'text'}, inplace=True)\n",
    "            input_cl = df[['cl', 'label']].copy()\n",
    "            input_cl.rename(columns={'cl': 'text'}, inplace=True)\n",
    "            input_text = pd.concat([input_tl, input_ab, input_cl]).copy()\n",
    "            input_text['text_len'] = input_text['text'].str.len()\n",
    "            input_text2 = input_text.loc[\n",
    "                input_text['text_len'] > 3, ['text', 'label']].copy()  # 60813 rows × 3 columns 제거\n",
    "            return input_text2\n",
    "\n",
    "        train_ds = make_input_text(train_input)\n",
    "        val_ds = make_input_text(val_input)\n",
    "        test_ds = make_input_text(test_input)\n",
    "\n",
    "        train_ds.to_csv('.cache/train_ds.tsv', encoding='utf-8', mode='w', index=False, sep='\\t')\n",
    "        val_ds.to_csv('.cache/val_ds.tsv', encoding='utf-8', mode='w', index=False, sep='\\t')\n",
    "        test_ds.to_csv('.cache/test_ds.tsv', encoding='utf-8', mode='w', index=False, sep='\\t')\n",
    "        print(time.strftime('%l:%M%p %Z on %b %d, %Y'))  # ' 1:36PM EDT on Oct 18, 2010'\n",
    "        print('Saving text-label pair dataset completed')\n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152b0a42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T06:44:25.501166Z",
     "start_time": "2022-11-24T06:44:01.939227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3:44PM KST on Nov 24, 2022\n",
      "Loading saved text-label pair dataset completed\n"
     ]
    }
   ],
   "source": [
    "train_ds = nlp.data.TSVDataset('.cache/train_ds.tsv', encoding='utf-8',\n",
    "                               field_indices=[0, 1], num_discard_samples=1)\n",
    "val_ds = nlp.data.TSVDataset('.cache/val_ds.tsv', encoding='utf-8',\n",
    "                             field_indices=[0, 1], num_discard_samples=1)\n",
    "test_ds = nlp.data.TSVDataset('.cache/test_ds.tsv', encoding='utf-8',\n",
    "                              field_indices=[0, 1], num_discard_samples=1)\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y'))  # ' 1:36PM EDT on Oct 18, 2010'\n",
    "print('Loading saved text-label pair dataset completed')\n",
    "with open('.cache/label_ksic.pickle', 'rb') as f:\n",
    "    ksic_index_dict = pickle.load(f)\n",
    "with open('.cache/ksic_label.pickle', 'rb') as f:\n",
    "    ksic_label_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a4c1e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-24T07:02:17.884592Z",
     "start_time": "2022-11-24T07:02:17.879833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108062\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(train_ds), len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d57c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3216bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in tqdm(dataset)]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in tqdm(dataset)]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4453e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256\n",
    "batch_size = 16\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 10000\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y'))  # 8:37AM KST on Jun 16, 2022\n",
    "print('Starting to make BERTDataset')\n",
    "try:\n",
    "    # data_train_id = np.load('.cache/data_train_id.npy', mmap_mode='r')\n",
    "    # data_train_id = np.load('.cache/ksic_data_train_id.npy', allow_pickle=True)\n",
    "    # data_test_id = np.load('.cache/ksic_data_test_id.npy', allow_pickle=True)\n",
    "    with open(\".cache/ksic_data_train_id.pickle\", \"rb\") as fr:\n",
    "        data_train_id = pickle.load(fr)\n",
    "    with open(\".cache/ksic_data_test_id.pickle\", \"rb\") as fr:\n",
    "        data_test_id = pickle.load(fr)\n",
    "    # Array can't be memory-mapped: Python objects in dtype. -> mmap_mode='r' 주석처리\n",
    "    # Object arrays cannot be loaded when allow_pickle=False -> np.dave에 allow_pickle=True 문구 추가\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y'))  # ' 1:36PM EDT on Oct 18, 2010'\n",
    "    print('Completed to load BERTDataset')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    train_ds, val_ds, test_ds = load_train_dataset()\n",
    "    data_train_id = BERTDataset(train_ds, 0, 1, tok, max_len, True, False)\n",
    "    # np.save('.cache/ksic_data_train_id.npy', data_train_id, allow_pickle=True)\n",
    "    with open('.cache/ksic_data_train_id.pickle', \"wb\") as fw:\n",
    "        pickle.dump(data_train_id, fw)\n",
    "    data_test_id = BERTDataset(test_ds, 0, 1, tok, max_len, True, False)\n",
    "    # np.save('.cache/ksic_data_test_id.npy', data_test_id, allow_pickle=True)\n",
    "    with open('.cache/ksic_data_test_id.pickle', \"wb\") as fw:\n",
    "        pickle.dump(data_test_id, fw)\n",
    "    # data_train_id = BERTDataset(train_ds.to_numpy(), 0, 1, tok, max_len, True, False)\n",
    "    # data_test_id = BERTDataset(test_ds.to_numpy(), 0, 1, tok, max_len, True, False)\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y'))  # ' 1:36PM EDT on Oct 18, 2010'\n",
    "    print('Completed to make BERTDataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6dce5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b85674b7adb4940a5985540a76dc6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특1994-021923\n",
      "특1994-016867\n",
      "특1995-016613\n",
      "특1995-017642\n",
      "특1995-017643\n",
      "특1994-012484\n",
      "특1993-019813\n",
      "특1994-022446\n",
      "특1994-027240\n",
      "특1991-002356\n",
      "특1994-013563\n",
      "특1993-028967\n",
      "특1994-003074\n",
      "특1990-020903\n",
      "특1990-019424\n",
      "특1994-028441\n",
      "특1994-033382\n",
      "특1990-015538\n",
      "특1989-012382\n",
      "특1994-001380\n",
      "특1989-009500\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train_id, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test_id, batch_size=batch_size, num_workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7264e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size=768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(),\n",
    "                              attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41bab707",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, num_classes=500, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0bae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0ba69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 ,  ['20411' '25121' '20491' '30110' '11111']\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c674f984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28119    16022\n",
      "26121    15241\n",
      "27112    14819\n",
      "Name: ksic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def calc_accuracy(X, y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    acc = (max_indices == y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b89ffd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 -> 500\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y'), ': starting epoch ', e)\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader),\n",
    "                                                                        total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"\\ntime: {}, epoch {} batch id {}\\n loss {} train acc {}\".format(time.strftime('%l:%M%p'),\n",
    "                                                                                   e+1, batch_id+1,\n",
    "                                                                                   loss.data.cpu().numpy(),\n",
    "                                                                                   train_acc / (batch_id+1)))\n",
    "            # torch.save(model, model_path)  # 전체 모델 저장\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader),\n",
    "                                                                        total=len(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    print(time.strftime('%l:%M%p %Z on %b %d, %Y'))\n",
    "    # torch.save(model, model_path)  # 전체 모델 저장\n",
    "    torch.save(model, os.path.join(model_path, 'KSIC_KoBERT.pt'))  # 전체 모델 저장\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, 'KSIC_model_state_dict.pt'))  # 모델 객체의 state_dict 저장\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }, os.path.join(model_path, 'all.tar'))\n",
    "    # 여러 가지 값 저장, 학습 중 진행 상황 저장을 위해 epoch, loss 값 등 일반 scalar값 저장 가능,\n",
    "    # https://velog.io/@dev-junku/KoBERT-%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%B4\n",
    "\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb931f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073105e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eafcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f41a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a80ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://velog.io/@dev-junku/KoBERT-%EB%AA%A8%EB%8D%B8%EC%97%90-%EB%8C%80%ED%95%B4\n",
    "PATH = 'drive/MyDrive/colab/StoryFlower/bert' # google 드라이브 연동 해야함. 관련코드는 뺐음\n",
    "torch.save(model, PATH + 'KoBERT_담화.pt')  # 전체 모델 저장\n",
    "torch.save(model.state_dict(), PATH + 'model_state_dict.pt')  # 모델 객체의 state_dict 저장\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, PATH + 'all.tar')  # 여러 가지 값 저장, 학습 중 진행 상황 저장을 위해 epoch, loss 값 등 일반 scalar값 저장 가능\n",
    "\n",
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==3.0.2\n",
    "!pip install torch\n",
    "\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#GPU 사용\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "#BERT 모델, Vocabulary 불러오기 필수\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "\n",
    "# KoBERT에 입력될 데이터셋 정리\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))  \n",
    "\n",
    "# 모델 정의\n",
    "class BERTClassifier(nn.Module): ## 클래스를 상속\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=6,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 20\n",
    "max_grad_norm = 1\n",
    "log_interval = 100\n",
    "learning_rate =  5e-5\n",
    "\n",
    "## 학습 모델 로드\n",
    "PATH = 'drive/MyDrive/colab/StoryFlower/bert/'\n",
    "model = torch.load(PATH + 'KoBERT_담화_86.pt')  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "model.load_state_dict(torch.load(PATH + 'model_state_dict_86.pt'))  # state_dict를 불러 온 후, 모델에 저장\n",
    "\n",
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "def new_softmax(a) : \n",
    "    c = np.max(a) # 최댓값\n",
    "    exp_a = np.exp(a-c) # 각각의 원소에 최댓값을 뺀 값에 exp를 취한다. (이를 통해 overflow 방지)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = (exp_a / sum_exp_a) * 100\n",
    "    return np.round(y, 3)\n",
    "\n",
    "\n",
    "# 예측 모델 설정\n",
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            min_v = min(logits)\n",
    "            total = 0\n",
    "            probability = []\n",
    "            logits = np.round(new_softmax(logits), 3).tolist()\n",
    "            for logit in logits:\n",
    "                print(logit)\n",
    "                probability.append(np.round(logit, 3))\n",
    "\n",
    "            if np.argmax(logits) == 0:  emotion = \"기쁨\"\n",
    "            elif np.argmax(logits) == 1: emotion = \"불안\"\n",
    "            elif np.argmax(logits) == 2: emotion = '당황'\n",
    "            elif np.argmax(logits) == 3: emotion = '슬픔'\n",
    "            elif np.argmax(logits) == 4: emotion = '분노'\n",
    "            elif np.argmax(logits) == 5: emotion = '상처'\n",
    "\n",
    "            probability.append(emotion)\n",
    "            print(probability)\n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06435cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60334648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader2 = torch.utils.data.DataLoader(data_test_id, batch_size=batch_size, num_workers=5)\n",
    "test_acc = 0.0\n",
    "results = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader2), total=len(test_dataloader2)):\n",
    "    print(batch_id)\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    max_vals, max_indices = torch.max(out, 1)\n",
    "    results.extend(max_indices.tolist())\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "    print(\"test acc {}\".format(test_acc / (batch_id+1)))\n",
    "print(results)\n",
    "#     print('label: ', label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d528d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_indices)\n",
    "print(max_indices.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe18709",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "exp = m(out)\n",
    "max_vals, max_indices = torch.max(out, 1)\n",
    "print(max_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae15239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dacon.io/en/competitions/official/235747/codeshare/3082?page=1&dtype=recent\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef28b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델 학습 과정 표시하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(loss.long().to(device), 'y', label='train loss')\n",
    "# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(train_acc.long().to(device), 'b', label='train_acc')\n",
    "acc_ax.plot(test_acc.long().to(device), 'g', label='test_acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(token_ids, valid_length, segment_ids)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be2cbc",
   "metadata": {},
   "source": [
    "100%\n",
    "2344/2344 [10:52<00:00, 3.82it/s]\n",
    "epoch 1 batch id 1 loss 0.7244999408721924 train acc 0.453125\n",
    "epoch 1 batch id 201 loss 0.4514557123184204 train acc 0.5766480099502488\n",
    "epoch 1 batch id 401 loss 0.44362887740135193 train acc 0.6851231296758105\n",
    "epoch 1 batch id 601 loss 0.5137903094291687 train acc 0.734426996672213\n",
    "epoch 1 batch id 801 loss 0.43659207224845886 train acc 0.7636352996254682\n",
    "epoch 1 batch id 1001 loss 0.3066664934158325 train acc 0.7802978271728271\n",
    "epoch 1 batch id 1201 loss 0.3117211163043976 train acc 0.7930890924229809\n",
    "epoch 1 batch id 1401 loss 0.30558276176452637 train acc 0.8026075124910778\n",
    "epoch 1 batch id 1601 loss 0.3304724097251892 train acc 0.8104992973141787\n",
    "epoch 1 batch id 1801 loss 0.25170841813087463 train acc 0.8164995141588006\n",
    "epoch 1 batch id 2001 loss 0.2823370695114136 train acc 0.8224481509245377\n",
    "epoch 1 batch id 2201 loss 0.3238280713558197 train acc 0.8274292935029532\n",
    "epoch 1 train acc 0.8307713843856656\n",
    "100%\n",
    "782/782 [01:06<00:00, 13.16it/s]\n",
    "epoch 1 test acc 0.8835118286445013\n",
    "100%\n",
    "2344/2344 [10:52<00:00, 3.81it/s]\n",
    "epoch 2 batch id 1 loss 0.4955632984638214 train acc 0.828125\n",
    "epoch 2 batch id 201 loss 0.2225867211818695 train acc 0.8816075870646766\n",
    "epoch 2 batch id 401 loss 0.33707231283187866 train acc 0.8836502493765586\n",
    "epoch 2 batch id 601 loss 0.39554905891418457 train acc 0.887115224625624\n",
    "epoch 2 batch id 801 loss 0.30988579988479614 train acc 0.8883426966292135\n",
    "epoch 2 batch id 1001 loss 0.28933045268058777 train acc 0.8911713286713286\n",
    "epoch 2 batch id 1201 loss 0.24474024772644043 train acc 0.8932269983347211\n",
    "epoch 2 batch id 1401 loss 0.1908964067697525 train acc 0.8957218058529621\n",
    "epoch 2 batch id 1601 loss 0.23474052548408508 train acc 0.89772993441599\n",
    "epoch 2 batch id 1801 loss 0.1599130779504776 train acc 0.8997171710161022\n",
    "epoch 2 batch id 2001 loss 0.20312610268592834 train acc 0.9019865067466267\n",
    "epoch 2 batch id 2201 loss 0.2386036366224289 train acc 0.9033251930940481\n",
    "epoch 2 train acc 0.904759047923777\n",
    "100%\n",
    "782/782 [01:06<00:00, 11.77it/s]\n",
    "epoch 2 test acc 0.8906449808184144\n",
    "100%\n",
    "2344/2344 [10:51<00:00, 3.81it/s]\n",
    "epoch 3 batch id 1 loss 0.3390277922153473 train acc 0.875\n",
    "epoch 3 batch id 201 loss 0.15983553230762482 train acc 0.9240516169154229\n",
    "epoch 3 batch id 401 loss 0.14856880903244019 train acc 0.9265118453865336\n",
    "epoch 3 batch id 601 loss 0.2642267644405365 train acc 0.9280366056572379\n",
    "epoch 3 batch id 801 loss 0.1951659917831421 train acc 0.93020443196005\n",
    "epoch 3 batch id 1001 loss 0.26453569531440735 train acc 0.9319586663336663\n",
    "epoch 3 batch id 1201 loss 0.1282612681388855 train acc 0.9340523522064946\n",
    "epoch 3 batch id 1401 loss 0.1837957799434662 train acc 0.9360501427551748\n",
    "epoch 3 batch id 1601 loss 0.14137345552444458 train acc 0.9374414428482198\n",
    "epoch 3 batch id 1801 loss 0.09849003702402115 train acc 0.9387493059411438\n",
    "epoch 3 batch id 2001 loss 0.15634101629257202 train acc 0.9401080709645178\n",
    "epoch 3 batch id 2201 loss 0.1982114464044571 train acc 0.9410495229441163\n",
    "epoch 3 train acc 0.9420039640216155\n",
    "100%\n",
    "782/782 [01:06<00:00, 11.77it/s]\n",
    "epoch 3 test acc 0.8969988810741688\n",
    "100%\n",
    "2344/2344 [10:52<00:00, 3.82it/s]\n",
    "epoch 4 batch id 1 loss 0.388761043548584 train acc 0.875\n",
    "epoch 4 batch id 201 loss 0.06205718219280243 train acc 0.9593439054726368\n",
    "epoch 4 batch id 401 loss 0.06854245811700821 train acc 0.9596711346633416\n",
    "epoch 4 batch id 601 loss 0.23485814034938812 train acc 0.9600925540765392\n",
    "epoch 4 batch id 801 loss 0.1342923790216446 train acc 0.9612008426966292\n",
    "epoch 4 batch id 1001 loss 0.1908232569694519 train acc 0.9621472277722277\n",
    "epoch 4 batch id 1201 loss 0.11091630905866623 train acc 0.9632597835137385\n",
    "epoch 4 batch id 1401 loss 0.10650145262479782 train acc 0.9642219842969307\n",
    "epoch 4 batch id 1601 loss 0.08601253479719162 train acc 0.9649242660836976\n",
    "epoch 4 batch id 1801 loss 0.057230446487665176 train acc 0.9656093836757357\n",
    "epoch 4 batch id 2001 loss 0.07411567866802216 train acc 0.9665011244377811\n",
    "epoch 4 batch id 2201 loss 0.1597125381231308 train acc 0.9671456156292594\n",
    "epoch 4 train acc 0.9676701151877133\n",
    "100%\n",
    "782/782 [01:06<00:00, 11.77it/s]\n",
    "epoch 4 test acc 0.8980778452685422\n",
    "100%\n",
    "2344/2344 [10:52<00:00, 3.81it/s]\n",
    "epoch 5 batch id 1 loss 0.3727969229221344 train acc 0.890625\n",
    "epoch 5 batch id 201 loss 0.02794063650071621 train acc 0.9752798507462687\n",
    "epoch 5 batch id 401 loss 0.024620698764920235 train acc 0.9767378428927681\n",
    "epoch 5 batch id 601 loss 0.15002880990505219 train acc 0.9765495008319468\n",
    "epoch 5 batch id 801 loss 0.05448848009109497 train acc 0.9766112671660424\n",
    "epoch 5 batch id 1001 loss 0.08006531745195389 train acc 0.9770541958041958\n",
    "epoch 5 batch id 1201 loss 0.04451199620962143 train acc 0.9775317443796836\n",
    "epoch 5 batch id 1401 loss 0.08561042696237564 train acc 0.9780625446109922\n",
    "epoch 5 batch id 1601 loss 0.026716381311416626 train acc 0.9783533728919426\n",
    "epoch 5 batch id 1801 loss 0.02442212402820587 train acc 0.9787357717934481\n",
    "epoch 5 batch id 2001 loss 0.0197431817650795 train acc 0.9790339205397302\n",
    "epoch 5 batch id 2201 loss 0.03802771866321564 train acc 0.9792565879145843\n",
    "epoch 5 train acc 0.9794199729806597\n",
    "100%\n",
    "782/782 [01:06<00:00, 11.78it/s]\n",
    "epoch 5 test acc 0.8974384590792839"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoBERT",
   "language": "python",
   "name": "kobert-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
