{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8d71b8",
   "metadata": {},
   "source": [
    "### KPC C09K11/06\n",
    "#### CPC 분류 코드\n",
    "* C09K11/06 : 1dot, containing organic luminescent materials (유기 발광성 물질을 함유하는 것)\n",
    "    * C09K11/00 : main group, Luminescent, e.g. electroluminescent, chemiluminescent materials (발광성 물질, 예. 전기 발광성 물질, 화학 발광성 물질)\n",
    "        * C09K : subclass, MATERIALS FOR MISCELLANEOUS APPLICATIONS, NOT PROVIDED FOR ELSEWHERE (그 밖에 분류되지 않는 다수의 응용을 위한 재료)\n",
    "            * C09 : class, DYES; PAINTS; POLISHES; NATURAL RESINS; ADHESIVES; COMPOSITIONS NOT OTHERWISE PROVIDED FOR; APPLICATIONS OF MATERIALS NOT OTHERWISE PROVIDED FOR (염료;  페인트;  광택제;  천연 수지;  접착제;  그 밖에 분류되지 않는 조성물;  그 밖에 분류되지 않는 재료의 응용)\n",
    "                * C : section, CHEMISTRY; METALLURGY (화학;  야금)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2259572",
   "metadata": {},
   "source": [
    "#### KPC 분류 단계\n",
    "* C09K11/06은 CPC이며, KPC는 K1부터 시작됨\n",
    "* decision #01 : CPC C09K11/06 vs 2dot entries(C09K11/06K1, C09K11/06K2)\n",
    "    * 만약 decision #01의 결과가 C09K11/06K2가 아니라면 분류 종료\n",
    "    * 만약 decision #01의 결과가 C09K11/06K2라면 decision #2\n",
    "        * decision #02 : CPC C09K11/06 vs 3dot entries(C09K11/06K21, C09K11/06K22, C09K11/06K23, C09K11/06K24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdec3c",
   "metadata": {},
   "source": [
    "#### C09K11/06 Scheme\n",
    "![](scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a12ac3",
   "metadata": {},
   "source": [
    "#### Decision Process\n",
    "![](C09K11-06.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1eddf56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 2.38 µs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5b7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from kobert import get_tokenizer\n",
    "from kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f723d24",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d4e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = nlp.data.TSVDataset('kpc_c09k/test_C09K11_220715.txt', field_indices=[0,1], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4079543b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"kpc_c09k/c09k_ind_label.pickle\", \"rb\") as fr:\n",
    "    kpc_index_dict = pickle.load(fr)\n",
    "with open(\"kpc_c09k/c09k_label_ind.pickle\", \"rb\") as fr:\n",
    "    kpc_label_dict = pickle.load(fr)\n",
    "num_class = len(kpc_index_dict)\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb48a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['보안 인쇄물의 위변조 확인 방법', '0'], ['보안 인쇄물의 위변조 확인 방법', '0'], ['  본 발명은 보안잉크가 인쇄된 보안 인쇄물에 관한 것으로서, 보다 상세하게는, 발광색상, 지속시간 또는 여기파장이 다른 형광체 또는 인광체를 포함하는 보안잉크가 인쇄된 보안 인쇄물에 관한 것이다. 이를 위해 보안 인쇄물은 제1영역 및 제2영역으로 구분되는 것으로서, 제1영역은 청색 형광체를 포함하는 보안잉크로 인쇄되고, 제2영역은 청색 형광체, 녹색 인광체 및 적색 인광체를 포함하는 보안잉크로 인쇄되는 것을 특징으로 한다.  ', '0'], ['  본 발명은 보안잉크가 인쇄된 보안 인쇄물에 관한 것으로서, 보다 상세하게는, 발광색상, 지속시간 또는 여기파장이 다른 형광체 또는 인광체를 포함하는 보안잉크가 인쇄된 보안 인쇄물에 관한 것이다. 이를 위해 보안 인쇄물은 제1영역 및 제2영역으로 구분되는 것으로서, 제1영역은 청색 형광체를 포함하는 보안잉크로 인쇄되고, 제2영역은 청색 형광체, 녹색 인광체 및 적색 인광체를 포함하는 보안잉크로 인쇄되는 것을 특징으로 한다.  ', '0'], ['UV 광원을 사용한 보안 인쇄물의 위변조 확인 방법에 있어서,피인쇄물을 준비하는 단계;UV 광원을 상기 제1영역 및 제2영역에 조사하여, 상기 제1영역은 제1색으로 발광하고, 제2영역은 제2색으로 발광하는 다색 발광 단계;UV 광원의 조사를 중지하는 단계; 및UV 조사가 중지된 이후, 상기 제1영역의 발광은 사라지고, 동시에 제2영역은 소정 시간 동안 제3색을 발광하고, 이후 제3색이 사라지거나, 제4색으로 일정 시간 동안 발광하는 색변환 단계;를 포함하고, 상기 피인쇄물은, 형광체을 포함하는 제1 보안 잉크를 사용하여 상기 피인쇄물의 표면에 제1영역이 인쇄되고, 형광체와 인광체을 포함하는 제2 보안 잉크를 사용하여 상기 피인쇄물의 표면에 제2영역이 인쇄된 것인, UV 광원을 사용한 보안 인쇄물의 위변조 확인 방법.', '0']]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfcc584",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f08355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")  ## CPU\n",
    "device = torch.device(\"cuda:0\")  ## GPU\n",
    "load_model_path = 'ksic_model'\n",
    "save_model_path1 = 'kpc_c09k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9671ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size=768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(),\n",
    "                              attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df894f",
   "metadata": {},
   "source": [
    "#### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e114b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KSIC 학습 모델을 불러와서 학습\n",
    "bertmodel = torch.load(os.path.join(load_model_path, 'KSIC_KoBERT.pt'))  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "# bertmodel.load_state_dict(\n",
    "#     torch.load(os.path.join(load_model_path, 'KSIC_model_state_dict.pt')))  # state_dict를 불러 온 후, 모델에 저장\n",
    "_, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6af4ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/hdh/PycharmProjects/KoBERT-master/.cache/kobert_v1.zip\n",
      "using cached model. /home/hdh/PycharmProjects/KoBERT-master/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(os.path.join(save_model_path1, 'kpc_c09k_KoBERT.pt'))  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "_, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n",
    "# bertmodel.load_state_dict(\n",
    "#     torch.load(os.path.join(save_model_path1, 'kpc_c09k_KoBERT.pt')))\n",
    "# torch.save(model, os.path.join(save_model_path1, 'kpc_c09k_KoBERT.pt'))  # 전체 모델 저장\n",
    "# torch.save(model.state_dict(), os.path.join(save_model_path1, 'kpc_c09k_model_state_dict.pt'))  # 모델 객체의 state_dict 저장\n",
    "# torch.save({\n",
    "#     'model': model.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "# }, os.path.join(save_model_path1, 'all.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT 모델을 불러와서 학습\n",
    "# bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a90eda0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac104a4",
   "metadata": {},
   "source": [
    "### GluonNLP Toolkit\n",
    "* GluonNLP Toolkit provides tools for building efficient data pipelines for NLP tasks.\n",
    "* https://nlp.gluon.ai/api/modules/data.html#gluonnlp.data.BERTSentenceTransform\n",
    "* class gluonnlp.data.BERTSPTokenizer(path, vocab, num_best=0, alpha=1.0, lower=True, max_input_chars_per_word=200)[source]¶\n",
    "* https://nlp.gluon.ai/api/modules/data.html#gluonnlp.data.TSVDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba861e5",
   "metadata": {},
   "source": [
    "### GluonNLP BERTSPTokenizer\n",
    "* BERTSPTokenizer depends on the sentencepiece library.\n",
    "* For multi-processing with BERTSPTokenizer, making an extra copy of the BERTSPTokenizer instance is recommended before using it.\n",
    "* https://nlp.gluon.ai/api/data.html?highlight=bertsptokenizer#gluonnlp.data.BERTSPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df56e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/hdh/PycharmProjects/KoBERT-master/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd92ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  본 발명은 보안잉크가 인쇄된 보안 인쇄물에 관한 것으로서, 보다 상세하게는, 발광색상, 지속시간 또는 여기파장이 다른 형광체 또는 인광체를 포함하는 보안잉크가 인쇄된 보안 인쇄물에 관한 것이다. 이를 위해 보안 인쇄물은 제1영역 및 제2영역으로 구분되는 것으로서, 제1영역은 청색 형광체를 포함하는 보안잉크로 인쇄되고, 제2영역은 청색 형광체, 녹색 인광체 및 적색 인광체를 포함하는 보안잉크로 인쇄되는 것을 특징으로 한다.  \n"
     ]
    }
   ],
   "source": [
    "print(dataset_test[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01745787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁본', '▁발', '명은', '▁보안', '잉', '크', '가', '▁인쇄', '된', '▁보안', '▁인쇄', '물', '에', '▁관한', '▁것으로', '서', '▁', ',', '▁보다', '▁상', '세', '하게', '는', '▁', ',', '▁발', '광', '색', '상', '▁', ',', '▁지속', '시간', '▁또는', '▁여기', '파', '장이', '▁다른', '▁형', '광', '체', '▁또는', '▁인', '광', '체', '를', '▁포함', '하는', '▁보안', '잉', '크', '가', '▁인쇄', '된', '▁보안', '▁인쇄', '물', '에', '▁관한', '▁것이다', '▁', '.', '▁이를', '▁위해', '▁보안', '▁인쇄', '물', '은', '▁제', '1', '영', '역', '▁및', '▁제', '2', '영', '역', '으로', '▁구', '분', '되는', '▁것으로', '서', '▁', ',', '▁제', '1', '영', '역', '은', '▁청', '색', '▁형', '광', '체', '를', '▁포함', '하는', '▁보안', '잉', '크', '로', '▁인쇄', '되고', '▁', ',', '▁제', '2', '영', '역', '은', '▁청', '색', '▁형', '광', '체', '▁', ',', '▁녹색', '▁인', '광', '체', '▁및', '▁적', '색', '▁인', '광', '체', '를', '▁포함', '하는', '▁보안', '잉', '크', '로', '▁인쇄', '되는', '▁것을', '▁특징', '으로', '▁한다', '▁', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_txt = dataset_test[2][0][:256]\n",
    "# sample_txt = 'GaN 전력 증폭기를 이용한 전력 오실레이터, 본 발명은 GaN(Gallium Nitride) 소자로 구성되며, 입력 신호의 전력을 증폭시켜 출력하는 GaN 전력 증폭기, 상기 GaN 전력 증폭기의 출력 신호의 일부를 피드백 신호로 제공하는 디렉셔널 커플러, 상기 디렉셔널 커플러에 의해서 제공되는 피드백 신호의 페이저를 가변시키는 페이저 시프터 및 상기 페이저 시프터에 의한 임피던스 부정합을 조정하며, 상기 GaN 전력 증폭기로 상기 피드백 신호를 전달하는 제 1 아이솔레이터를 포함하는 GaN 전력 증폭기를 이용한 전력 오실레이터가 제공된다.'\n",
    "print(tok(sample_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bd1e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 512\n",
    "batch_size = 8\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 1000\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0f38b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_sample_text(ind):\n",
    "    sample_txt = dataset_test[ind][0][:max_len]\n",
    "    print(dataset_test[ind][0])\n",
    "    print(tok(sample_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecde47ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 양태에서, 본 개시내용은 본 개시내용의 화합물의 배합물을 제공한다. 2. 양태에서, 본 개시내용은 본 개시내용의 화합물을 포함하는 유기층을 갖는 OLED를 제공한다. 3. 양태에서, 본 개시내용은 본 개시내용의 화합물을 포함하는 유기층을 갖는 OLED를 포함하는 소비자 제품을 제공한다. 4. 양태에서, 본 개시내용은 또한 본 개시내용의 상기 화합물 섹션에서 개시된 화합물을 함유하는 제1 유기층을 포함하는 OLED 디바이스를 제공한다. 5. 양태에서, OLED는 애노드, 캐소드, 및 애노드와 캐소드 사이에 배치된 제1 유기층을 포함한다. 6. 양태에서, 유기층은 호스트를 더 포함할 수 있고, 호스트는 트리페닐렌, 카르바졸, 인돌로카르바졸, 디벤조티오펜, 디벤조푸란, 디벤조셀레노펜, 5,9-디옥사-13b-보라나프토[3,2,1-de]안트라센, 아자-트리페닐렌, 아자-카르바졸, 아자-인돌로카르바졸, 아자-디벤조티오펜, 아자-디벤조푸란, 아자-디벤조셀레노펜, 및 아자-(5,9-디옥사-13b-보라나프토[3,2,1-de]안트라센)으로 이루어진 군으로부터 선택되는 1 이상의 화학기를 포함한다. 7. 양태에서, 호스트는 하기 화합물 및 이들의 조합으로 이루어진 호스트 그룹으로부터 선택될 수 있다: [0207] [0208] [0209] 일부 실시양태에서, 유기층은 호스트를 더 포함할 수 있고, 호스트는 금속 착물을 포함한다. 8. 양태에서, OLED는 탄소 나노튜브를 포함하는 층을 더 포함한다. 9. 양태에서, OLED는 지연 형광 이미터를 포함하는 층을 더 포함한다. 10. 양태에서, OLED는 RGB 픽셀 배열, 또는 화이트 플러스 컬러 필터 픽셀 배열을 포함한다. 11. 양태에서, ETL에 사용되는 화합물은 분자에서 하기 기 중 하나 이상을 포함한다: [0335] [0336] 여기서 R101은 수소, 중수소, 할로겐, 알킬, 시클로알킬, 헤테로알킬, 헤테로시클로알킬, 아릴알킬, 알콕시, 아릴옥시, 아미노, 실릴, 알케닐, 시클로알케닐, 헤테로알케닐, 알키닐, 아릴, 헤테로아릴, 아실, 카르복실산, 에테르, 에스테르, 니트릴, 이소니트릴, 술파닐, 술피닐, 술포닐, 포스피노 및 이들의 조합으로 이루어진 군으로부터 선택되며, 아릴 또는 헤테로아릴인 경우, 상기 기술한 Ar과 유사한 정의를 가진다.\n",
      "['▁1', '▁', '.', '▁양', '태', '에서', '▁', ',', '▁본', '▁개', '시', '내', '용', '은', '▁본', '▁개', '시', '내', '용', '의', '▁화', '합', '물', '의', '▁배', '합', '물을', '▁제공한다', '▁', '.', '▁2', '▁', '.', '▁양', '태', '에서', '▁', ',', '▁본', '▁개', '시', '내', '용', '은', '▁본', '▁개', '시', '내', '용', '의', '▁화', '합', '물을', '▁포함', '하는', '▁유', '기', '층', '을', '▁갖', '는', '▁O', 'LED', '를', '▁제공한다', '▁', '.', '▁3', '▁', '.', '▁양', '태', '에서', '▁', ',', '▁본', '▁개', '시', '내', '용', '은', '▁본', '▁개', '시', '내', '용', '의', '▁화', '합', '물을', '▁포함', '하는', '▁유', '기', '층', '을', '▁갖', '는', '▁O', 'LED', '를', '▁포함', '하는', '▁소비자', '▁제품을', '▁제공한다', '▁', '.', '▁4', '▁', '.', '▁양', '태', '에서', '▁', ',', '▁본', '▁개', '시', '내', '용', '은', '▁또한', '▁본', '▁개', '시', '내', '용', '의', '▁상', '기', '▁화', '합', '물', '▁', '섹', '션', '에서', '▁개', '시', '된', '▁화', '합', '물을', '▁함', '유', '하는', '▁제', '1', '▁유', '기', '층', '을', '▁포함', '하는', '▁O', 'LED', '▁디', '바이', '스를', '▁제공한다', '▁', '.', '▁5', '▁', '.', '▁양', '태', '에서', '▁', ',', '▁O', 'LED', '는', '▁애', '노', '드', '▁', ',', '▁캐', '소', '드', '▁', ',', '▁및', '▁애', '노', '드', '와', '▁캐', '소', '드', '▁사이에', '▁배치', '된', '▁제', '1', '▁유', '기', '층', '을', '▁포함한', '다', '▁', '.', '▁6', '▁', '.', '▁양', '태', '에서', '▁', ',', '▁유', '기', '층', '은', '▁호', '스트', '를', '▁더', '▁포함', '할', '▁수', '▁있고', '▁', ',', '▁호', '스트', '는', '▁', '트리', '페', '닐', '렌', '▁', ',', '▁카', '르', '바', '졸', '▁', ',', '▁인', '돌', '로', '카', '르', '바', '졸', '▁', ',', '▁디', '벤', '조', '티', '오', '펜', '▁', ',', '▁디', '벤', '조', '푸', '란', '▁', ',', '▁디', '벤', '조', '셀', '레', '노', '펜', '▁', ',', '▁5', '▁', ',', '▁9', '▁-', '▁디', '옥', '사', '▁-', '▁13', 'b', '▁-', '▁보', '라', '나', '프', '토', '▁[', '▁3', '▁', ',', '▁2', '▁', ',', '▁1', '▁-', '▁', 'd', 'e', '▁', ']', '▁안', '트', '라', '센', '▁', ',', '▁아', '자', '▁-', '▁', '트리', '페', '닐', '렌', '▁', ',', '▁아', '자', '▁-', '▁카', '르', '바', '졸', '▁', ',', '▁아', '자', '▁-', '▁인', '돌', '로', '카', '르', '바', '졸', '▁', ',', '▁아', '자', '▁-', '▁디', '벤', '조', '티', '오', '펜', '▁', ',', '▁아', '자', '▁-', '▁디', '벤', '조', '푸', '란', '▁', ',', '▁아', '자', '▁-', '▁디', '벤', '조', '셀', '레', '노', '펜', '▁', ',', '▁및', '▁아', '자', '▁-', '▁(', '▁5', '▁', ',', '▁9', '▁-', '▁디', '옥', '사', '▁-', '▁13', 'b', '▁-', '▁보', '라', '나', '프', '토', '▁[', '▁3', '▁', ',', '▁2']\n",
      "1. 발명이 해결하고자 하는 과제는 신규한 방향족 화합물 및 이를 포함하여 저전압 구동이 가능하고, 우수한 발광효율 및 수명 특성을 갖는 유기전계발광소자를 제공하는 것이다.\n",
      "['▁1', '▁', '.', '▁발', '명이', '▁해결', '하고', '자', '▁하는', '▁과제', '는', '▁신규', '한', '▁방향', '족', '▁화', '합', '물', '▁및', '▁이를', '▁포함', '하여', '▁저', '전', '압', '▁구', '동', '이', '▁가능', '하고', '▁', ',', '▁우수', '한', '▁발', '광', '효율', '▁및', '▁수', '명', '▁특성', '을', '▁갖', '는', '▁유', '기', '전', '계', '발', '광', '소', '자를', '▁제공하는', '▁것이다', '▁', '.']\n",
      "  화학식 Ir(L1)x(L2)y(L3)z의 화합물이 제공되며, 상기 식에서, L1은, 화학식 I 을 갖고; L1은 Ir을 갖는 5-원 킬레이트를 형성하고; L2 및 L3는 동일하거나 상이할 수 있고, 화학식 II 을 갖는다.  \n",
      "['▁화학', '식', '▁I', 'r', '▁(', '▁L', '1', '▁', ')', '▁', 'x', '▁(', '▁L', '2', '▁', ')', '▁', 'y', '▁(', '▁L', '3', '▁', ')', '▁', 'z', '의', '▁화', '합', '물', '이', '▁제공', '되며', '▁', ',', '▁상', '기', '▁식', '에서', '▁', ',', '▁L', '1', '은', '▁', ',', '▁화학', '식', '▁I', '▁', '을', '▁갖고', '▁', ';', '▁L', '1', '은', '▁I', 'r', '을', '▁갖', '는', '▁5', '▁-', '▁원', '▁', '킬', '레이', '트', '를', '▁형성', '하고', '▁', ';', '▁L', '2', '▁및', '▁L', '3', '는', '▁동', '일', '하거나', '▁상', '이', '할', '▁수', '▁있고', '▁', ',', '▁화학', '식', '▁', 'II', '▁', '을', '▁갖', '는', '다', '▁', '.']\n",
      "화합물 및 이를 포함하는 유기 발광 소자 1. [0438] 본 명세서에 있어서, 상기 '층'은 본 기술분야에 주로 사용되는 '필름'과 호환되는 의미이며, 목적하는 영역을 덮는 코팅을 의미한다.\n",
      "['▁화', '합', '물', '▁및', '▁이를', '▁포함', '하는', '▁유', '기', '▁발', '광', '▁소', '자', '▁1', '▁', '.', '▁[', '▁', '04', '38', '▁', ']', '▁본', '▁명', '세', '서', '에', '▁있어서', '▁', ',', '▁상', '기', \"▁'\", '▁', '층', \"▁'\", '▁', '은', '▁본', '▁기술', '분야', '에', '▁주로', '▁사용', '되는', \"▁'\", '▁', '필름', \"▁'\", '▁', '과', '▁호', '환', '되는', '▁의미', '이며', '▁', ',', '▁목적', '하는', '▁영역', '을', '▁', '덮', '는', '▁코', '팅', '을', '▁의미', '한다', '▁', '.']\n",
      "화합물 및 이를 포함하는 유기 발광 소자 1. 하기 화학식 1로 표시되는 화합물:[화학식 1]    상기 화학식 1에 있어서,L2는 직접결합; 치환 또는 비치환된 알킬렌기; 치환 또는 비치환된 아릴렌기; 또는 치환 또는 비치환된 헤테로아릴렌기이고, Ar21은 치환 또는 비치환된 아릴기이고,HAr21 및 HAr22는 서로 동일하거나 상이하고, 각각 독립적으로 치환 또는 비치환된 헤테로고리기이고,HAr21과 -L2-HAr22는 동시에 치환 또는 비치환된 피리딜기가 아니다.\n",
      "['▁화', '합', '물', '▁및', '▁이를', '▁포함', '하는', '▁유', '기', '▁발', '광', '▁소', '자', '▁1', '▁', '.', '▁하기', '▁화학', '식', '▁1', '로', '▁표시', '되는', '▁화', '합', '물', '▁:', '▁[', '▁화학', '식', '▁1', '▁', ']', '▁상', '기', '▁화학', '식', '▁1', '에', '▁있어서', '▁', ',', '▁L', '2', '는', '▁직접', '결', '합', '▁', ';', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁알', '킬', '렌', '기', '▁', ';', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁아', '릴', '렌', '기', '▁', ';', '▁또는', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁헤', '테', '로', '아', '릴', '렌', '기', '이', '고', '▁', ',', '▁A', 'r', '21', '은', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁아', '릴', '기', '이', '고', '▁', ',', '▁H', 'A', 'r', '21', '▁및', '▁H', 'A', 'r', '22', '는', '▁서로', '▁동', '일', '하거나', '▁상', '이', '하고', '▁', ',', '▁각각', '▁독립', '적으로', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁헤', '테', '로', '고', '리', '기', '이', '고', '▁', ',', '▁H', 'A', 'r', '21', '과', '▁-', '▁L', '2', '▁-', '▁H', 'A', 'r', '22', '는', '▁동시에', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁피', '리', '딜', '기가', '▁아니다', '▁', '.']\n",
      "1. 발명의 또 다른 구체 예에서, 상기 유기물층은 스핀코팅 공정, 노즐 프린팅 공정, 잉크젯 프린팅 공정, 슬롯코팅 공정, 딥코팅 공정 및 롤투롤 공정 중 어느 하나에 의해 형성됨을 특징으로 하는 유기전기소자를 제공한다.\n",
      "['▁1', '▁', '.', '▁발', '명의', '▁또', '▁다른', '▁구', '체', '▁예', '에서', '▁', ',', '▁상', '기', '▁유', '기', '물', '층', '은', '▁스', '핀', '코', '팅', '▁공정', '▁', ',', '▁노', '즐', '▁', '프', '린', '팅', '▁공정', '▁', ',', '▁', '잉', '크', '젯', '▁', '프', '린', '팅', '▁공정', '▁', ',', '▁슬', '롯', '코', '팅', '▁공정', '▁', ',', '▁', '딥', '코', '팅', '▁공정', '▁및', '▁', '롤', '투', '롤', '▁공정', '▁중', '▁어느', '▁하나', '에', '▁의해', '▁형성', '됨', '을', '▁특징', '으로', '▁하는', '▁유', '기', '전', '기', '소', '자를', '▁제공한다', '▁', '.']\n",
      "유기 발광 소자 1. 발명에 따른 유기 발광 소자는 애노드; 캐소드; 및 상기 애노드 및 상기 캐소드 사이에 구비된 유기물층을 포함하고, 상기 유기물층은 발광층 및 제1 유기물층을 포함하고, 상기 제1 유기물층은 상기 애노드 및 상기 발광층 사이에 구비되고, 상기 발광층은 상기 화학식 1의 화합물을 포함하고, 상기 제1 유기물층은 상기 화학식 2의 화합물을 포함하는 것을 특징으로 한다. 2. 발명의 화학식 1을 포함하는 발광층 및 화학식 2을 포함하는 제1 유기물층을 포함하는 소자는 고효율, 저전압 및 장수명의 특징이 있음을 알 수 있다.\n",
      "['▁유', '기', '▁발', '광', '▁소', '자', '▁1', '▁', '.', '▁발', '명', '에', '▁따른', '▁유', '기', '▁발', '광', '▁소', '자는', '▁애', '노', '드', '▁', ';', '▁캐', '소', '드', '▁', ';', '▁및', '▁상', '기', '▁애', '노', '드', '▁및', '▁상', '기', '▁캐', '소', '드', '▁사이에', '▁구', '비', '된', '▁유', '기', '물', '층', '을', '▁포함', '하고', '▁', ',', '▁상', '기', '▁유', '기', '물', '층', '은', '▁발', '광', '층', '▁및', '▁제', '1', '▁유', '기', '물', '층', '을', '▁포함', '하고', '▁', ',', '▁상', '기', '▁제', '1', '▁유', '기', '물', '층', '은', '▁상', '기', '▁애', '노', '드', '▁및', '▁상', '기', '▁발', '광', '층', '▁사이에', '▁구', '비', '되고', '▁', ',', '▁상', '기', '▁발', '광', '층', '은', '▁상', '기', '▁화학', '식', '▁1', '의', '▁화', '합', '물을', '▁포함', '하고', '▁', ',', '▁상', '기', '▁제', '1', '▁유', '기', '물', '층', '은', '▁상', '기', '▁화학', '식', '▁2', '의', '▁화', '합', '물을', '▁포함', '하는', '▁것을', '▁특징', '으로', '▁한다', '▁', '.', '▁2', '▁', '.', '▁발', '명의', '▁화학', '식', '▁1', '을', '▁포함', '하는', '▁발', '광', '층', '▁및', '▁화학', '식', '▁2', '을', '▁포함', '하는', '▁제', '1', '▁유', '기', '물', '층', '을', '▁포함', '하는', '▁소', '자는', '▁고', '효율', '▁', ',', '▁저', '전', '압', '▁및', '▁장', '수', '명의', '▁특징', '이', '▁있음', '을', '▁알', '▁수', '▁있다', '▁', '.']\n",
      "하기 [화학식 1]로 표시되는 이형고리 화합물:[화학식 1]    상기 [화학식 1]에서,A1 내지 A4 중에서 선택되는 인접한 두 개;와, A5 내지 A8 중에서 선택되는 인접한 두 개; 중에서 어느 하나가 하기 [구조식 1] 또는 [구조식 2]와 결합하거나,A1 내지 A4 중에서 선택되는 인접한 두 개;와 A5 내지 A8 중에서 선택되는 인접한 두 개;가 각각 하기 [구조식 1] 또는 [구조식 2]와 결합하여 축합고리를 형성하는 것을 특징으로 하고,상기 A1 내지 A8 중에서 [구조식 1] 또는 [구조식 2]와 결합하지 않는 것은 모두 CR이고, 상기 R은 수소, 치환 또는 비치환된 탄소수 2 내지 30의 헤테로아릴기 및 치환 또는 비치환된 탄소수 6 내지 30의 아릴기 중에서 선택되며,[구조식 1]   [구조식 2]      상기 [구조식 1]과 [구조식 2]에서,'*'은 상기 [화학식 1]의 A1 내지 A4 중에서 선택되는 인접한 두 개;와 A5 내지 A8 중에서 선택되는 인접한 두 개;와 결합하는 부위를 나타낸 것이고,Y1은 -CR1R2-, -NR3-, -SiR4R5-, -O- 또는 -S-이고, Y2는 -N-이며, Z는 -CR1R2-이고 (n은 0 내지 1의 정수임),상기 R1 내지 R5는 치환 또는 비치환된 탄소수 1 내지 30의 알킬기, 치환 또는 비치환된 탄소수 6 내지 40의 아릴기 및 치환 또는 비치환된 탄소수 2 내지 30의 헤테로아릴기 중에서 선택되며,X1 내지 X7은 서로 동일하거나 상이하며 각각 독립적으로 수소, 중수소, 할로겐 원자, 치환 또는 비치환된 탄소수 1 내지 30의 알킬기, 치환 또는 비치환된 탄소수 6 내지 40의 아릴기, 치환 또는 비치환된 탄소수 2 내지 30의 헤테로아릴기 및 치환 또는 비치환된 탄소수 3 내지 60의 시클로알킬기 중에서 선택되며,단, 상기 [화학식 1]로 표시되는 이형고리 화합물은 하기 구조식으로 표시되는 구조인 경우는 제외한다.       \n",
      "['▁하기', '▁[', '▁화학', '식', '▁1', '▁', ']', '▁', '로', '▁표시', '되는', '▁이', '형', '고', '리', '▁화', '합', '물', '▁:', '▁[', '▁화학', '식', '▁1', '▁', ']', '▁상', '기', '▁[', '▁화학', '식', '▁1', '▁', ']', '▁', '에서', '▁', ',', '▁A', '1', '▁내', '지', '▁A', '4', '▁중', '에서', '▁선택', '되는', '▁인', '접', '한', '▁두', '▁개', '▁', ';', '▁', '와', '▁', ',', '▁A', '5', '▁내', '지', '▁A', '8', '▁중', '에서', '▁선택', '되는', '▁인', '접', '한', '▁두', '▁개', '▁', ';', '▁중', '에서', '▁어느', '▁하나', '가', '▁하기', '▁[', '▁구조', '식', '▁1', '▁', ']', '▁또는', '▁[', '▁구조', '식', '▁2', '▁', ']', '▁', '와', '▁결합', '하거나', '▁', ',', '▁A', '1', '▁내', '지', '▁A', '4', '▁중', '에서', '▁선택', '되는', '▁인', '접', '한', '▁두', '▁개', '▁', ';', '▁', '와', '▁A', '5', '▁내', '지', '▁A', '8', '▁중', '에서', '▁선택', '되는', '▁인', '접', '한', '▁두', '▁개', '▁', ';', '▁', '가', '▁각각', '▁하기', '▁[', '▁구조', '식', '▁1', '▁', ']', '▁또는', '▁[', '▁구조', '식', '▁2', '▁', ']', '▁', '와', '▁결합', '하여', '▁축', '합', '고', '리를', '▁형성', '하는', '▁것을', '▁특징', '으로', '▁하고', '▁', ',', '▁상', '기', '▁A', '1', '▁내', '지', '▁A', '8', '▁중', '에서', '▁[', '▁구조', '식', '▁1', '▁', ']', '▁또는', '▁[', '▁구조', '식', '▁2', '▁', ']', '▁', '와', '▁결합', '하지', '▁않는', '▁것은', '▁모두', '▁C', 'R', '이', '고', '▁', ',', '▁상', '기', '▁R', '은', '▁수', '소', '▁', ',', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁탄', '소', '수', '▁2', '▁내', '지', '▁30', '의', '▁헤', '테', '로', '아', '릴', '기', '▁및', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁탄', '소', '수', '▁6', '▁내', '지', '▁30', '의', '▁아', '릴', '기', '▁중', '에서', '▁선택', '되며', '▁', ',', '▁[', '▁구조', '식', '▁1', '▁', ']', '▁[', '▁구조', '식', '▁2', '▁', ']', '▁상', '기', '▁[', '▁구조', '식', '▁1', '▁', ']', '▁', '과', '▁[', '▁구조', '식', '▁2', '▁', ']', '▁', '에서', '▁', ',', \"▁'\", '▁*', \"▁'\", '▁', '은', '▁상', '기', '▁[', '▁화학', '식', '▁1', '▁', ']', '▁', '의', '▁A', '1', '▁내', '지', '▁A', '4', '▁중', '에서', '▁선택', '되는', '▁인', '접', '한', '▁두', '▁개', '▁', ';', '▁', '와', '▁A', '5', '▁내', '지', '▁A', '8', '▁중', '에서', '▁선택', '되는', '▁인', '접', '한', '▁두', '▁개', '▁', ';', '▁', '와', '▁결합', '하']\n",
      "유기 일렉트로루미네센스 소자, 유기 일렉트로루미네센스 소자의 제조 방법, 표시 장치 및 조명 장치 1. 발명의 유기 일렉트로루미네센스 소자는, 적어도 1쌍의 양극과 음극에 의해 끼워진 유기층을 함유하는 유기 일렉트로루미네센스 소자이며, 상기 유기층이 발광층을 포함하는 적어도 1층으로 이루어지고, 당해 유기층 중 적어도 1층이 하기 일반식 (A1) 내지 (A5)로 표시되는 화합물 중 적어도 하나를 함유하는 것을 특징으로 한다. 2. 발명에 사용되는 인광 도펀트는, 임의의 용매 중 어느 하나에 있어서 상기 인광 양자 수율(0.01 이상)이 달성되면 된다.\n",
      "['▁유', '기', '▁일', '렉', '트로', '루', '미', '네', '센스', '▁소', '자', '▁', ',', '▁유', '기', '▁일', '렉', '트로', '루', '미', '네', '센스', '▁소', '자의', '▁제조', '▁방법', '▁', ',', '▁표시', '▁', '장치', '▁및', '▁조명', '▁', '장치', '▁1', '▁', '.', '▁발', '명의', '▁유', '기', '▁일', '렉', '트로', '루', '미', '네', '센스', '▁소', '자는', '▁', ',', '▁적어', '도', '▁1', '쌍', '의', '▁양', '극', '과', '▁음', '극', '에', '▁의해', '▁끼', '워', '진', '▁유', '기', '층', '을', '▁함', '유', '하는', '▁유', '기', '▁일', '렉', '트로', '루', '미', '네', '센스', '▁소', '자', '이며', '▁', ',', '▁상', '기', '▁유', '기', '층', '이', '▁발', '광', '층', '을', '▁포함', '하는', '▁적어', '도', '▁1', '층', '으로', '▁이루', '어', '지고', '▁', ',', '▁당', '해', '▁유', '기', '층', '▁중', '▁적어', '도', '▁1', '층', '이', '▁하기', '▁일반', '식', '▁(', '▁A', '1', '▁', ')', '▁내', '지', '▁(', '▁A', '5', '▁', ')', '▁', '로', '▁표시', '되는', '▁화', '합', '물', '▁중', '▁적어', '도', '▁하나', '를', '▁함', '유', '하는', '▁것을', '▁특징', '으로', '▁한다', '▁', '.', '▁2', '▁', '.', '▁발', '명', '에', '▁사용', '되는', '▁인', '광', '▁', '도', '펀', '트', '는', '▁', ',', '▁임', '의', '의', '▁용', '매', '▁중', '▁어느', '▁하나', '에', '▁있어서', '▁상', '기', '▁인', '광', '▁양', '자', '▁수', '율', '▁(', '▁0', '▁', '.', '▁', '01', '▁이상', '▁', ')', '▁이', '▁달성', '되면', '▁된다', '▁', '.']\n",
      "\"1. [0049] 본 명세서에 있어서, 상기 \\\"\"층\\\"\"은 본 기술분야에 주로 사용되는 \\\"\"필름\\\"\"과 호환되는 의미이며, 목적하는 영역을 덮는 코팅을 의미한다.\"\n",
      "['▁\"', '▁1', '▁', '.', '▁[', '▁', '00', '49', '▁', ']', '▁본', '▁명', '세', '서', '에', '▁있어서', '▁', ',', '▁상', '기', '▁', '\\\\', '▁\"', '▁\"', '▁', '층', '▁', '\\\\', '▁\"', '▁\"', '▁', '은', '▁본', '▁기술', '분야', '에', '▁주로', '▁사용', '되는', '▁', '\\\\', '▁\"', '▁\"', '▁', '필름', '▁', '\\\\', '▁\"', '▁\"', '▁', '과', '▁호', '환', '되는', '▁의미', '이며', '▁', ',', '▁목적', '하는', '▁영역', '을', '▁', '덮', '는', '▁코', '팅', '을', '▁의미', '한다', '▁', '.', '▁\"']\n"
     ]
    }
   ],
   "source": [
    "lst = np.random.randint(0, len(dataset_test), 10)\n",
    "for i in lst:\n",
    "    view_sample_text(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "675246bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = ['▁화', '합', '물', '▁및', '▁이를', '▁포함', '하는', '▁유', '기', '▁발', '광', '▁소', '자', '▁하기', '▁화학', '식', '▁1', '로', '▁표시', '되는', '▁화', '합', '물', '▁:', '▁[', '▁화학', '식', '▁1', '▁', ']', '▁상', '기', '▁화학', '식', '▁1', '에', '▁있어서', '▁', ',', '▁L', '1', '▁및', '▁L', '2', '는', '▁서로', '▁같', '거나', '▁상', '이', '하고', '▁', ',', '▁각각', '▁독립', '적으로', '▁직접', '결', '합', '▁', ';', '▁또는', '▁아', '릴', '렌', '기', '이며', '▁', ',', '▁A', 'r', '1', '▁및', '▁A', 'r', '2', '는', '▁서로', '▁같', '거나', '▁상', '이', '하고', '▁', ',', '▁각각', '▁독립', '적으로', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁N', '▁포함', '▁단', '환', '의', '▁헤', '테', '로', '고', '리', '기', '▁', ';', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁N', '포', '함', '▁6', '원', '고', '리', '로', '▁이루', '어', '진', '▁2', '환', '의', '▁헤', '테', '로', '고', '리', '기', '▁', ';', '▁또는', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁O', '▁또는', '▁S', '를', '▁포함', '하는', '▁헤', '테', '로', '고', '리', '기', '이다', '▁', '.']\n",
    "len(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13899827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "#         self.labels = [i[label_idx] for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f23e2",
   "metadata": {},
   "source": [
    "#### max_len vs batch size\n",
    "* Text examples should be mostly less than 512 tokens. Longer texts will be cut from the end to fit the sequence length specified in the model block.\n",
    "* https://peltarion.com/knowledge-center/documentation/cheat-sheets/bert---text-classification-/-cheat-sheet\n",
    "* Sequence length\tRecommended max batch size\n",
    "    * 64 - 64, 128 - 32, 256 - 16, 320 - 14, 384 - 12, 512 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4dfca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_id = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5aee963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   2,  529,  517,   54, 2235, 6210, 3014, 7828, 5081, 5481, 5112,\n",
      "       7842, 6242, 5112, 7842, 6241, 7095, 3214, 1958, 5788, 6896, 3093,\n",
      "       7147, 7096, 5920, 5573, 4864, 7810,  517,   46, 5081, 5481,  784,\n",
      "       5859, 5330, 3508, 7828, 4788, 7096, 3862,  517,   54,    3,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1], dtype=int32), array(43, dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print(data_test_id.sentences[162])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b63ab828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(data_test_id.labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88dc73d",
   "metadata": {},
   "source": [
    "* DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False)\n",
    "* https://pytorch.org/docs/stable/data.html\n",
    "* WARNING\n",
    "    * After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is number of workers * size of parent process). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out issue #13246 for more details on why this occurs and example code for how to workaround these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9df4a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(data_test_id, batch_size=batch_size, num_workers=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BERTClassifier(bertmodel, num_classes=num_class, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69bd2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/KoBERT-master/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(optimizer_grouped_parameters, lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m      8\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 9\u001b[0m t_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_dataloader\u001b[49m) \u001b[38;5;241m*\u001b[39m num_epochs\n\u001b[1;32m     10\u001b[0m warmup_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(t_total \u001b[38;5;241m*\u001b[39m warmup_ratio)\n\u001b[1;32m     11\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_cosine_schedule_with_warmup(optimizer, num_warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_step, num_training_steps\u001b[38;5;241m=\u001b[39mt_total)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2895ab36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506ec8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y')) # ' 1:36PM EDT on Oct 18, 2010'\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        val_loss = loss_fn(out, label)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} val_loss {} test acc {}\".format(e+1, val_loss, test_acc / (batch_id+1)))\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y')) # ' 1:36PM EDT on Oct 18, 2010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9949130d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6325914b46c3463f847ef43afe6ee4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/651 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc 0.23623911930363542\n",
      "[10, 10, 9, 9, 9, 9, 3, 3, 9, 9, 9, 9, 9, 9, 13, 13, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 9, 9, 9, 9, 9, 9, 17, 17, 9, 9, 3, 9, 17, 17, 13, 5, 3, 3, 5, 1, 3, 4, 1, 3, 1, 1, 1, 1, 17, 17, 5, 5, 3, 5, 5, 5, 3, 5, 5, 5, 9, 5, 17, 17, 9, 5, 17, 9, 9, 9, 9, 9, 9, 9, 9, 17, 17, 17, 17, 9, 9, 1, 1, 11, 11, 17, 17, 17, 9, 1, 1, 11, 11, 1, 1, 13, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 13, 9, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 17, 2, 2, 2, 2, 17, 2, 2, 2, 7, 7, 8, 8, 6, 6, 3, 3, 7, 7, 7, 7, 7, 7, 7, 7, 17, 17, 2, 2, 2, 1, 2, 2, 2, 1, 17, 17, 2, 1, 2, 2, 2, 2, 17, 17, 2, 2, 2, 17, 2, 2, 2, 17, 17, 17, 2, 2, 2, 2, 2, 2, 17, 17, 3, 17, 3, 17, 17, 17, 17, 17, 17, 3, 17, 3, 17, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 7, 7, 7, 5, 7, 7, 7, 7, 7, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 5, 5, 4, 4, 16, 16, 4, 10, 5, 5, 4, 10, 7, 7, 4, 4, 4, 4, 5, 5, 4, 4, 16, 16, 4, 10, 5, 5, 4, 10, 7, 7, 4, 4, 4, 4, 5, 5, 4, 4, 16, 16, 4, 10, 5, 5, 4, 10, 7, 7, 4, 4, 4, 4, 5, 5, 4, 4, 16, 16, 4, 10, 5, 5, 4, 10, 7, 7, 4, 4, 4, 10, 5, 5, 4, 10, 16, 16, 4, 16, 5, 5, 4, 16, 7, 7, 4, 4, 4, 10, 5, 5, 4, 10, 16, 16, 4, 16, 5, 5, 4, 16, 7, 7, 4, 4, 4, 10, 5, 5, 4, 10, 16, 16, 4, 16, 5, 5, 4, 16, 7, 7, 4, 4, 4, 10, 5, 5, 4, 10, 16, 16, 4, 16, 5, 5, 4, 16, 7, 7, 4, 4, 4, 16, 5, 5, 4, 16, 16, 16, 4, 16, 5, 5, 4, 16, 7, 7, 4, 4, 4, 16, 5, 5, 4, 16, 16, 16, 4, 16, 5, 5, 4, 16, 7, 7, 4, 4, 4, 16, 5, 5, 4, 16, 16, 16, 4, 16, 5, 5, 4, 16, 7, 7, 4, 4, 4, 16, 5, 5, 4, 16, 16, 16, 4, 16, 5, 5, 4, 16, 7, 3, 5, 3, 5, 7, 4, 7, 4, 7, 3, 5, 3, 5, 7, 4, 7, 4, 7, 3, 5, 3, 5, 7, 4, 7, 4, 7, 3, 5, 3, 5, 7, 4, 7, 4, 3, 7, 7, 5, 7, 7, 7, 4, 7, 3, 7, 7, 5, 7, 7, 7, 4, 7, 3, 7, 7, 5, 7, 7, 7, 4, 7, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 3, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 3, 4, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 6, 9, 6, 7, 6, 9, 6, 7, 7, 6, 9, 6, 7, 6, 9, 6, 7, 7, 6, 9, 6, 7, 6, 9, 6, 7, 7, 6, 9, 6, 7, 6, 9, 6, 7, 7, 6, 9, 6, 7, 6, 9, 6, 7, 7, 6, 9, 6, 7, 6, 9, 6, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 7, 4, 7, 12, 7, 4, 7, 4, 7, 7, 4, 7, 12, 7, 4, 7, 4, 7, 7, 4, 7, 12, 7, 4, 7, 4, 7, 7, 11, 11, 5, 5, 11, 11, 7, 7, 11, 11, 5, 5, 11, 11, 7, 7, 11, 11, 5, 5, 11, 11, 7, 7, 11, 11, 5, 5, 11, 11, 7, 7, 11, 11, 5, 5, 11, 11, 7, 7, 11, 11, 5, 5, 11, 11, 7, 10, 4, 13, 5, 6, 7, 13, 9, 7, 10, 4, 13, 5, 6, 7, 13, 9, 7, 10, 4, 13, 5, 6, 7, 13, 9, 7, 10, 4, 13, 5, 6, 7, 13, 9, 7, 10, 4, 13, 5, 6, 7, 13, 9, 7, 10, 4, 13, 5, 6, 7, 13, 9, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 5, 4, 7, 5, 4, 16, 7, 5, 7, 5, 4, 7, 5, 4, 16, 7, 5, 7, 5, 4, 7, 5, 4, 16, 7, 5, 7, 5, 4, 7, 5, 4, 16, 7, 5, 4, 4, 4, 4, 7, 16, 4, 4, 12, 12, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 16, 4, 4, 12, 12, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 16, 4, 4, 12, 12, 4, 4, 4, 4, 4, 4, 4, 4, 7, 4, 4, 7, 5, 5, 16, 7, 4, 7, 4, 4, 7, 5, 5, 16, 7, 4, 7, 4, 4, 7, 5, 5, 16, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 13, 13, 12, 12, 12, 12, 9, 9, 12, 12, 12, 12, 12, 12, 13, 13, 12, 12, 13, 13, 12, 12, 12, 12, 9, 9, 12, 12, 12, 12, 12, 12, 13, 13, 12, 12, 13, 13, 12, 12, 12, 12, 9, 9, 12, 12, 12, 12, 12, 12, 13, 13, 12, 12, 13, 13, 12, 12, 12, 12, 9, 9, 12, 12, 12, 12, 12, 12, 13, 13, 12, 12, 12, 12, 5, 11, 12, 12, 5, 12, 12, 12, 5, 11, 12, 12, 5, 12, 12, 12, 5, 11, 12, 12, 5, 12, 12, 12, 5, 11, 12, 12, 5, 12, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 7, 7, 7, 5, 7, 7, 7, 9, 7, 9, 9, 9, 5, 7, 9, 7, 9, 7, 9, 9, 9, 5, 7, 9, 7, 9, 7, 9, 9, 9, 5, 7, 9, 7, 9, 7, 9, 9, 9, 5, 7, 9, 7, 9, 7, 9, 9, 9, 5, 7, 9, 7, 9, 7, 9, 9, 9, 5, 7, 9, 7, 9, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 9, 7, 4, 4, 7, 4, 4, 4, 7, 7, 4, 4, 7, 4, 4, 4, 7, 7, 4, 4, 7, 4, 4, 4, 7, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 9, 9, 9, 7, 5, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 9, 9, 7, 5, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 9, 9, 7, 5, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 9, 9, 7, 5, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 9, 9, 7, 5, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 9, 9, 7, 5, 9, 9, 7, 7, 9, 9, 7, 7, 9, 9, 7, 7, 7, 7, 16, 16, 7, 13, 7, 7, 7, 16, 16, 16, 7, 16, 10, 10, 7, 16, 7, 7, 16, 16, 7, 13, 7, 7, 7, 16, 16, 16, 7, 16, 10, 10, 7, 16, 7, 7, 7, 7, 7, 7, 10, 10, 7, 7, 7, 7, 7, 10, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 10, 10, 7, 7, 7, 7, 7, 10, 7, 7, 7, 7, 16, 7, 7, 12, 10, 7, 7, 16, 16, 16, 7, 7, 12, 10, 7, 7, 16, 16, 16, 7, 7, 11, 10, 7, 7, 16, 14, 16, 7, 7, 11, 10, 7, 7, 16, 14, 16, 16, 12, 5, 12, 16, 16, 12, 16, 9, 14, 7, 9, 7, 7, 7, 9, 7, 9, 14, 7, 9, 7, 7, 7, 9, 7, 7, 4, 7, 11, 7, 4, 7, 9, 7, 7, 4, 7, 11, 7, 4, 7, 9, 7, 7, 4, 7, 11, 7, 4, 7, 9, 7, 7, 4, 7, 11, 7, 4, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 16, 14, 16, 16, 16, 7, 16, 16, 7, 16, 14, 16, 16, 16, 7, 16, 16, 7, 9, 11, 9, 13, 9, 7, 9, 13, 7, 9, 11, 9, 13, 9, 7, 9, 13, 7, 9, 11, 9, 13, 9, 7, 9, 13, 7, 9, 11, 9, 13, 9, 7, 9, 13, 7, 9, 11, 9, 13, 9, 7, 9, 13, 7, 13, 6, 7, 9, 7, 6, 7, 9, 7, 13, 6, 7, 9, 7, 6, 7, 9, 7, 13, 6, 7, 9, 7, 6, 7, 9, 14, 14, 14, 14, 7, 7, 7, 7, 7, 7, 14, 14, 7, 7, 7, 7, 12, 12, 12, 12, 1, 1, 7, 7, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 12, 12, 12, 12, 1, 1, 7, 7, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 14, 7, 14, 7, 7, 14, 15, 6, 15, 11, 14, 17, 11, 11, 11, 4, 11, 11, 14, 17, 11, 11, 11, 4, 11, 11, 14, 17, 11, 11, 11, 4, 11, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 9, 3, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 11, 12, 4, 4, 14, 14, 15, 15, 16, 16, 15, 15, 14, 14, 15, 15, 16, 16, 15, 15, 7, 7, 4, 4, 7, 7, 16, 16, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 7, 7, 16, 16, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 14, 16, 14, 7, 14, 7, 14, 7, 7, 14, 16, 14, 7, 14, 7, 14, 7, 7, 14, 16, 14, 7, 14, 7, 14, 4, 4, 7, 4, 4, 4, 7, 4, 4, 4, 4, 7, 4, 4, 4, 7, 4, 4, 4, 4, 7, 4, 4, 4, 7, 4, 4, 4, 4, 7, 4, 4, 4, 7, 4, 4, 14, 14, 14, 14, 7, 7, 14, 14, 11, 11, 14, 14, 11, 11, 14, 14, 11, 11, 14, 14, 14, 14, 7, 7, 14, 14, 11, 11, 14, 14, 11, 11, 14, 14, 11, 11, 14, 14, 14, 14, 7, 7, 14, 14, 11, 11, 14, 14, 11, 11, 14, 14, 11, 11, 14, 14, 14, 14, 7, 7, 14, 14, 11, 11, 14, 14, 11, 11, 14, 14, 11, 11, 3, 7, 1, 7, 7, 7, 12, 7, 3, 3, 7, 1, 7, 7, 7, 12, 7, 3, 3, 1, 10, 7, 7, 17, 16, 7, 3, 3, 1, 10, 7, 7, 17, 16, 7, 3, 7, 10, 7, 10, 7, 7, 7, 7, 7, 7, 10, 7, 10, 7, 7, 7, 7, 7, 7, 10, 7, 10, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 14, 14, 15, 14, 14, 14, 14, 14, 14, 14, 15, 14, 14, 14, 14, 7, 11, 7, 16, 12, 12, 16, 16, 12, 7, 11, 7, 16, 12, 12, 16, 16, 12, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 16, 16, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 16, 16, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 16, 16, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 7, 14, 7, 9, 7, 7, 14, 3, 14, 11, 10, 16, 11, 11, 16, 11, 16, 11, 10, 16, 11, 11, 16, 11, 16, 16, 16, 11, 16, 14, 16, 14, 16, 16, 16, 11, 16, 14, 16, 14, 16, 16, 16, 11, 16, 14, 16, 14, 16, 16, 16, 11, 16, 14, 16, 14, 16, 16, 16, 11, 16, 14, 16, 14, 16, 11, 16, 16, 11, 11, 16, 11, 12, 11, 16, 16, 11, 11, 16, 11, 12, 11, 16, 16, 11, 11, 16, 11, 12, 6, 6, 6, 6, 13, 13, 6, 6, 13, 13, 13, 13, 13, 13, 6, 6, 13, 13, 6, 6, 6, 6, 13, 13, 6, 6, 13, 13, 13, 13, 13, 13, 6, 6, 13, 13, 6, 6, 6, 6, 13, 13, 6, 6, 13, 13, 13, 13, 13, 13, 6, 6, 13, 13, 7, 1, 7, 1, 9, 1, 7, 17, 9, 7, 1, 7, 1, 9, 1, 7, 17, 9, 7, 1, 7, 1, 9, 1, 7, 17, 9, 11, 12, 11, 12, 11, 16, 11, 12, 11, 12, 11, 12, 11, 16, 11, 12, 11, 12, 11, 12, 11, 16, 11, 12, 16, 11, 11, 16, 14, 16, 14, 16, 16, 11, 11, 16, 14, 16, 14, 16, 16, 11, 11, 16, 14, 16, 14, 16, 16, 16, 11, 16, 14, 12, 14, 16, 16, 16, 11, 16, 14, 12, 14, 16, 16, 16, 11, 16, 14, 12, 14, 16, 16, 16, 10, 7, 16, 16, 16, 7, 16, 16, 16, 14, 16, 14, 16, 14, 16, 14, 16, 16, 14, 16, 14, 16, 14, 16, 14, 16, 16, 16, 16, 12, 16, 16, 16, 16, 16, 16, 16, 16, 12, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4, 16, 16, 16, 16, 16, 16, 16, 16, 4, 16, 16, 16, 16, 16, 16, 16, 16, 4, 16, 16, 16, 16, 16, 16, 16, 16, 4, 16, 16, 7, 14, 7, 14, 7, 14, 7, 14, 7, 14, 7, 14, 7, 14, 7, 14, 7, 14, 7, 14, 7, 14, 7, 14, 7, 7, 6, 6, 10, 7, 10, 10, 16, 7, 6, 6, 7, 7, 7, 7, 7, 7, 3, 4, 16, 4, 16, 5, 4, 10, 10, 3, 4, 16, 4, 16, 5, 4, 10, 10, 3, 4, 16, 4, 16, 5, 4, 10, 10, 3, 4, 16, 4, 16, 5, 4, 10, 10, 16, 16, 16, 16, 16, 16, 3, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4, 4, 7, 7, 10, 10, 16, 16, 7, 7, 4, 4, 10, 10, 4, 4, 3, 3, 4, 4, 7, 7, 10, 10, 16, 16, 7, 7, 4, 4, 10, 10, 4, 4, 3, 3, 4, 4, 7, 7, 10, 10, 16, 16, 7, 7, 4, 4, 10, 10, 4, 4, 3, 3, 6, 6, 16, 6, 16, 13, 6, 6, 6, 6, 6, 16, 6, 16, 13, 6, 6, 6, 6, 6, 16, 6, 16, 13, 6, 6, 6, 3, 3, 16, 16, 4, 4, 7, 7, 1, 1, 16, 16, 4, 4, 7, 7, 7, 7, 3, 3, 16, 16, 4, 4, 7, 7, 1, 1, 16, 16, 4, 4, 7, 7, 7, 7, 3, 3, 16, 16, 4, 4, 7, 7, 1, 1, 16, 16, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 12, 12, 9, 13, 9, 9, 9, 9, 12, 12, 13, 9, 7, 7, 9, 9, 7, 7, 12, 12, 9, 13, 9, 9, 9, 9, 12, 12, 13, 9, 7, 7, 9, 9, 7, 7, 12, 12, 9, 13, 9, 9, 9, 9, 12, 12, 13, 9, 7, 7, 9, 9, 7, 7, 16, 16, 4, 4, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 7, 7, 7, 7, 16, 16, 4, 4, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 7, 7, 7, 7, 16, 16, 4, 4, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 7, 7, 9, 1, 1, 9, 1, 1, 1, 7, 1, 7, 16, 7, 16, 7, 16, 7, 16, 7, 7, 16, 7, 16, 7, 16, 7, 16, 7, 7, 3, 7, 12, 7, 7, 7, 16, 7, 7, 3, 7, 12, 7, 7, 7, 16, 7, 7, 3, 7, 12, 7, 7, 7, 16, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 4, 4, 5, 4, 16, 4, 5, 4, 7, 4, 4, 5, 4, 16, 4, 5, 4, 3, 3, 4, 3, 5, 3, 4, 3, 5, 7, 7, 6, 7, 6, 7, 6, 7, 7, 7, 7, 6, 7, 6, 7, 6, 7, 7, 7, 7, 6, 7, 6, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 6, 7, 7, 7, 6, 7, 7, 10, 11, 11, 11, 11, 7, 10, 7, 10, 11, 11, 11, 11, 7, 10, 7, 10, 11, 11, 11, 11, 7, 10, 7, 16, 16, 10, 16, 10, 16, 10, 16, 10, 16, 16, 10, 16, 10, 16, 10, 16, 10, 16, 16, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 7, 7, 9, 16, 10, 12, 16, 10, 13, 10, 13, 6, 6, 6, 10, 6, 6, 6, 7, 6, 6, 6, 10, 6, 6, 6, 7, 4, 4, 12, 12, 12, 12, 4, 4, 3, 3, 12, 12, 12, 12, 4, 4, 4, 4, 4, 4, 12, 12, 12, 12, 4, 4, 3, 3, 12, 12, 12, 12, 4, 4, 4, 4, 11, 11, 11, 11, 11, 11, 12, 11, 11, 11, 11, 11, 11, 11, 12, 11, 11, 11, 11, 11, 11, 11, 12, 11, 16, 16, 7, 16, 12, 16, 7, 16, 4, 16, 16, 7, 16, 12, 16, 7, 16, 4, 4, 10, 7, 16, 4, 4, 7, 16, 4, 10, 7, 16, 4, 4, 7, 16, 9, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 11, 9, 9, 9, 11, 9, 9, 7, 9, 11, 9, 9, 9, 11, 9, 9, 7, 9, 11, 9, 9, 9, 11, 9, 9, 7, 9, 11, 9, 9, 9, 11, 9, 9, 7, 16, 7, 16, 7, 16, 7, 16, 14, 7, 7, 16, 16, 3, 3, 16, 16, 7, 7, 16, 16, 7, 7, 16, 16, 16, 16, 16, 16, 16, 16, 3, 3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 7, 16, 7, 16, 7, 16, 7, 16, 16, 7, 16, 7, 16, 7, 16, 7, 16, 16, 16, 16, 5, 13, 10, 10, 9, 9, 16, 16, 13, 13, 16, 16, 9, 9, 7, 7, 16, 7, 16, 7, 16, 7, 16, 16, 16, 16, 16, 16, 11, 12, 12, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 16, 7, 7, 4, 16, 7, 7, 4, 17, 3, 2, 4, 17, 3, 2, 7, 4, 17, 3, 2, 4, 17, 3, 2, 4, 7, 7, 4, 7, 4, 7, 4, 7, 4, 7, 7, 4, 7, 4, 7, 4, 7, 16, 16, 7, 16, 12, 16, 7, 16, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 12, 12, 4, 12, 12, 12, 11, 12, 11, 7, 4, 4, 5, 7, 4, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 7, 6, 7, 6, 7, 7, 7, 7, 6, 7, 6, 7, 6, 7, 7, 16, 16, 16, 16, 16, 16, 10, 16, 10, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 7, 16, 12, 16, 7, 16, 4, 16, 16, 7, 16, 7, 16, 15, 16, 7, 9, 7, 7, 9, 7, 7, 7, 9, 7, 9, 7, 7, 9, 7, 7, 7, 9, 7, 16, 16, 16, 7, 12, 16, 16, 7, 4, 3, 4, 4, 12, 4, 4, 3, 12, 4, 3, 4, 4, 12, 4, 4, 3, 12, 4, 3, 4, 4, 12, 4, 4, 3, 12, 4, 3, 4, 4, 12, 4, 4, 3, 12, 4, 4, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 16, 12, 4, 4, 16, 12, 4, 4, 4, 4, 4, 16, 16, 4, 4, 10, 10, 7, 7, 4, 10, 4, 4, 7, 7, 12, 11, 12, 11, 4, 12, 12, 12, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 16, 16, 5, 4, 16, 16, 5, 4, 16, 16, 4, 4, 16, 16, 4, 4, 7, 7, 16, 16, 4, 5, 16, 16, 4, 5, 16, 16, 4, 5, 16, 16, 4, 5, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 4, 4, 3, 3, 4, 4, 7, 7, 7, 7, 4, 16, 7, 7, 4, 7, 7, 7, 7, 7, 5, 5, 7, 7, 7, 7, 7, 7, 4, 16, 7, 7, 4, 7, 7, 7, 7, 7, 5, 5, 7, 7, 7, 7, 7, 5, 14, 7, 7, 7, 7, 7, 7, 7, 5, 14, 7, 7, 7, 7, 4, 4, 16, 4, 4, 4, 4, 4, 4, 12, 16, 12, 16, 12, 12, 12, 12, 12, 4, 16, 4, 16, 16, 16, 12, 4, 16, 4, 4, 4, 3, 16, 4, 16, 4, 16, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 5, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 7, 7, 5, 5, 4, 4, 16, 16, 3, 7, 5, 5, 3, 3, 4, 4, 4, 4, 5, 5, 12, 12, 4, 4, 16, 16, 3, 7, 5, 5, 3, 3, 4, 4, 4, 4, 5, 5, 12, 12, 7, 16, 7, 16, 16, 16, 7, 16, 16, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 7, 9, 7, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 7, 9, 9, 9, 16, 9, 3, 9, 7, 7, 9, 9, 9, 16, 9, 3, 9, 7, 13, 13, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 13, 13, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 7, 7, 7, 9, 9, 7, 7, 13, 9, 7, 9, 9, 9, 7, 9, 9, 7, 7, 9, 9, 5, 13, 9, 9, 9, 9, 9, 9, 5, 13, 9, 9, 9, 9, 7, 9, 9, 9, 7, 9, 9, 9, 7, 7, 9, 9, 9, 7, 9, 9, 9, 7, 7, 9, 9, 9, 9, 9, 9, 7, 7, 7, 9, 9, 9, 9, 9, 9, 7, 7, 9, 9, 9, 9, 4, 9, 9, 9, 9, 9, 9, 9, 9, 4, 9, 9, 9, 9, 9, 9, 7, 9, 7, 9, 7, 9, 7, 9, 9, 7, 9, 7, 9, 7, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 5, 7, 5, 7, 9, 7, 13, 7, 7, 7, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 9, 7, 9, 7, 9, 7, 9, 7, 9, 9, 9, 3, 9, 9, 9, 9, 9, 7, 9, 11, 9, 9, 7, 11, 9, 7, 7, 7, 7, 9, 9, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 7, 1, 7, 9, 7, 16, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 9, 7, 7, 7, 9, 7, 9, 9, 9, 3, 9, 9, 9, 9, 9, 9, 9, 9, 3, 9, 9, 9, 9, 9, 9, 9, 16, 9, 9, 9, 12, 9, 9, 9, 16, 9, 9, 9, 12, 9, 9, 9, 9, 3, 9, 9, 9, 9, 9, 9, 9, 9, 3, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 13, 13, 13, 13, 13, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 7, 9, 7, 9, 7, 9, 7, 7, 9, 7, 9, 7, 9, 7, 9, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 9, 9, 9, 9, 9, 9, 9, 3, 9, 9, 9, 9, 9, 9, 9, 9, 3, 9, 9, 9, 9, 9, 7, 7, 9, 9, 7, 9, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader2 = torch.utils.data.DataLoader(data_test_id, batch_size=batch_size, num_workers=5)\n",
    "test_acc = 0.0\n",
    "results = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader2), total=len(test_dataloader2)):\n",
    "#     print(batch_id)\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    max_vals, max_indices = torch.max(out, 1)\n",
    "    results.extend(max_indices.tolist())\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "print(\"test acc {}\".format(test_acc / (batch_id+1)))\n",
    "print(results)\n",
    "#     print('label: ', label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3671f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.tensor(results)\n",
    "target = torch.tensor(data_test_id.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f2fb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionmatrix = ConfusionMatrix(num_classes=num_class)\n",
    "test_matrix = confusionmatrix(preds, target)\n",
    "test_matrix_df = pd.DataFrame(test_matrix)\n",
    "test_matrix_df.to_csv(os.path.join(save_model_path1, 'confusionmatrix.csv'))\n",
    "# tensor([[2, 0],\n",
    "#         [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d528d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'K0',\n",
       " 1: 'K1',\n",
       " 2: 'K21',\n",
       " 3: 'K241',\n",
       " 4: 'K2133',\n",
       " 5: 'K211',\n",
       " 6: 'K2123',\n",
       " 7: 'K2132',\n",
       " 8: 'K214',\n",
       " 9: 'K23',\n",
       " 10: 'K2122',\n",
       " 11: 'K242',\n",
       " 12: 'K212',\n",
       " 13: 'K24',\n",
       " 14: 'K2131',\n",
       " 15: 'K2121',\n",
       " 16: 'K213',\n",
       " 17: 'K22'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpc_label_dict\n",
    "kpc_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbe18709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 100,\n",
       " 9: 766,\n",
       " 3: 135,\n",
       " 13: 106,\n",
       " 17: 54,\n",
       " 5: 303,\n",
       " 1: 108,\n",
       " 4: 691,\n",
       " 11: 296,\n",
       " 16: 573,\n",
       " 2: 34,\n",
       " 7: 1543,\n",
       " 8: 2,\n",
       " 6: 121,\n",
       " 12: 204,\n",
       " 14: 152,\n",
       " 15: 15}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict = {}\n",
    "for y in results:\n",
    "    if y in pred_dict.keys():\n",
    "        pred_dict[y] += 1\n",
    "    else:\n",
    "        pred_dict[y] = 1\n",
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd8af5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K0': 0,\n",
       " 'K1': 1,\n",
       " 'K21': 2,\n",
       " 'K241': 3,\n",
       " 'K2133': 4,\n",
       " 'K211': 5,\n",
       " 'K2123': 6,\n",
       " 'K2132': 7,\n",
       " 'K214': 8,\n",
       " 'K23': 9,\n",
       " 'K2122': 10,\n",
       " 'K242': 11,\n",
       " 'K212': 12,\n",
       " 'K24': 13,\n",
       " 'K2131': 14,\n",
       " 'K2121': 15,\n",
       " 'K213': 16,\n",
       " 'K22': 17}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpc_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7700f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'K0',\n",
       " 1: 'K1',\n",
       " 2: 'K2',\n",
       " 3: 'K2',\n",
       " 4: 'K2',\n",
       " 5: 'K2',\n",
       " 6: 'K2',\n",
       " 7: 'K2',\n",
       " 8: 'K2',\n",
       " 9: 'K2',\n",
       " 10: 'K2',\n",
       " 11: 'K2',\n",
       " 12: 'K2',\n",
       " 13: 'K2',\n",
       " 14: 'K2',\n",
       " 15: 'K2',\n",
       " 16: 'K2',\n",
       " 17: 'K2'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpc_index_dict_2dot = {}\n",
    "kpc_label_dict_2dot_only = {'K0':0, 'K1':1, 'K2':2}\n",
    "for k in kpc_index_dict.keys():\n",
    "    kpc_index_dict_2dot[k] = kpc_index_dict[k][:2]\n",
    "kpc_index_dict_2dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ae15239",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2dot = [kpc_index_dict_2dot[k] for k in results]\n",
    "results_2dot = [kpc_label_dict_2dot_only[k] for k in results_2dot]\n",
    "results_2dot = torch.from_numpy(np.array(results_2dot))\n",
    "# np.unique(results_2dot)\n",
    "target_original = data_test_id.labels\n",
    "target_2dot = [kpc_index_dict_2dot[k] for k in target_original]\n",
    "target_2dot = [kpc_label_dict_2dot_only[k] for k in target_2dot]\n",
    "target_2dot = torch.from_numpy(np.array(target_2dot))\n",
    "# np.unique(target_2dot)\n",
    "confusionmatrix_2dot = ConfusionMatrix(num_classes=3)\n",
    "test_matrix_2dot = confusionmatrix_2dot(results_2dot, target_2dot)\n",
    "test_matrix_2dot_df = pd.DataFrame(test_matrix_2dot)\n",
    "test_matrix_2dot_df.to_csv(os.path.join(save_model_path1, 'confusionmatrix_2dot.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fef28b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kpc_index_dict_3dot = {}\n",
    "kpc_label_dict_3dot_only = {'K0':0, 'K1':1, 'K21':2, 'K22':3, 'K23':4, 'K24':5}\n",
    "for k in kpc_index_dict.keys():\n",
    "    kpc_index_dict_3dot[k] = kpc_index_dict[k][:3]\n",
    "kpc_index_dict_3dot\n",
    "results_3dot = [kpc_index_dict_3dot[k] for k in results]\n",
    "results_3dot = [kpc_label_dict_3dot_only[k] for k in results_3dot]\n",
    "results_3dot = torch.from_numpy(np.array(results_3dot))\n",
    "# np.unique(results_3dot)\n",
    "target_original = data_test_id.labels\n",
    "target_3dot = [kpc_index_dict_3dot[k] for k in target_original]\n",
    "target_3dot = [kpc_label_dict_3dot_only[k] for k in target_3dot]\n",
    "target_3dot = torch.from_numpy(np.array(target_3dot))\n",
    "# np.unique(target_3dot)\n",
    "confusionmatrix_3dot = ConfusionMatrix(num_classes=6)\n",
    "test_matrix_3dot = confusionmatrix_3dot(results_3dot, target_3dot)\n",
    "test_matrix_3dot_df = pd.DataFrame(test_matrix_3dot)\n",
    "test_matrix_3dot_df.to_csv(os.path.join(save_model_path1, 'confusionmatrix_3dot.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d441a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpc_index_dict_4dot = {}\n",
    "kpc_label_dict_4dot_only = {'K0':0, 'K1':1, 'K21':2, 'K211':3, 'K212':4, 'K213':5, 'K214':6, 'K22':7, 'K23':8, 'K24':9, 'K241':10, 'K242':11}\n",
    "for k in kpc_index_dict.keys():\n",
    "    kpc_index_dict_4dot[k] = kpc_index_dict[k][:4]\n",
    "# kpc_index_dict_4dot\n",
    "results_4dot = [kpc_index_dict_4dot[k] for k in results]\n",
    "results_4dot = [kpc_label_dict_4dot_only[k] for k in results_4dot]\n",
    "results_4dot = torch.from_numpy(np.array(results_4dot))\n",
    "# np.unique(results_4dot)\n",
    "target_original = data_test_id.labels\n",
    "target_4dot = [kpc_index_dict_4dot[k] for k in target_original]\n",
    "target_4dot = [kpc_label_dict_4dot_only[k] for k in target_4dot]\n",
    "target_4dot = torch.from_numpy(np.array(target_4dot))\n",
    "# np.unique(target_3dot)\n",
    "confusionmatrix_4dot = ConfusionMatrix(num_classes=len(kpc_label_dict_4dot_only))\n",
    "test_matrix_4dot = confusionmatrix_4dot(results_4dot, target_4dot)\n",
    "test_matrix_4dot_df = pd.DataFrame(test_matrix_4dot)\n",
    "test_matrix_4dot_df.to_csv(os.path.join(save_model_path1, 'confusionmatrix_4dot.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d52c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, os.path.join(save_model_path1, 'KSIC_KoBERT.pt'))  # 전체 모델 저장\n",
    "# torch.save(model.state_dict(), os.path.join(save_model_path1, 'KSIC_model_state_dict.pt'))  # 모델 객체의 state_dict 저장\n",
    "# torch.save({\n",
    "#     'model': model.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "# }, os.path.join(save_model_path1, 'all.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(token_ids, valid_length, segment_ids)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ee6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.82it/s]\n",
    "# epoch 1 batch id 1 loss 0.7244999408721924 train acc 0.453125\n",
    "# epoch 1 batch id 201 loss 0.4514557123184204 train acc 0.5766480099502488\n",
    "# epoch 1 batch id 401 loss 0.44362887740135193 train acc 0.6851231296758105\n",
    "# epoch 1 batch id 601 loss 0.5137903094291687 train acc 0.734426996672213\n",
    "# epoch 1 batch id 801 loss 0.43659207224845886 train acc 0.7636352996254682\n",
    "# epoch 1 batch id 1001 loss 0.3066664934158325 train acc 0.7802978271728271\n",
    "# epoch 1 batch id 1201 loss 0.3117211163043976 train acc 0.7930890924229809\n",
    "# epoch 1 batch id 1401 loss 0.30558276176452637 train acc 0.8026075124910778\n",
    "# epoch 1 batch id 1601 loss 0.3304724097251892 train acc 0.8104992973141787\n",
    "# epoch 1 batch id 1801 loss 0.25170841813087463 train acc 0.8164995141588006\n",
    "# epoch 1 batch id 2001 loss 0.2823370695114136 train acc 0.8224481509245377\n",
    "# epoch 1 batch id 2201 loss 0.3238280713558197 train acc 0.8274292935029532\n",
    "# epoch 1 train acc 0.8307713843856656\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 13.16it/s]\n",
    "# epoch 1 test acc 0.8835118286445013\n",
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.81it/s]\n",
    "# epoch 2 batch id 1 loss 0.4955632984638214 train acc 0.828125\n",
    "# epoch 2 batch id 201 loss 0.2225867211818695 train acc 0.8816075870646766\n",
    "# epoch 2 batch id 401 loss 0.33707231283187866 train acc 0.8836502493765586\n",
    "# epoch 2 batch id 601 loss 0.39554905891418457 train acc 0.887115224625624\n",
    "# epoch 2 batch id 801 loss 0.30988579988479614 train acc 0.8883426966292135\n",
    "# epoch 2 batch id 1001 loss 0.28933045268058777 train acc 0.8911713286713286\n",
    "# epoch 2 batch id 1201 loss 0.24474024772644043 train acc 0.8932269983347211\n",
    "# epoch 2 batch id 1401 loss 0.1908964067697525 train acc 0.8957218058529621\n",
    "# epoch 2 batch id 1601 loss 0.23474052548408508 train acc 0.89772993441599\n",
    "# epoch 2 batch id 1801 loss 0.1599130779504776 train acc 0.8997171710161022\n",
    "# epoch 2 batch id 2001 loss 0.20312610268592834 train acc 0.9019865067466267\n",
    "# epoch 2 batch id 2201 loss 0.2386036366224289 train acc 0.9033251930940481\n",
    "# epoch 2 train acc 0.904759047923777\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.77it/s]\n",
    "# epoch 2 test acc 0.8906449808184144\n",
    "# 100%\n",
    "# 2344/2344 [10:51<00:00, 3.81it/s]\n",
    "# epoch 3 batch id 1 loss 0.3390277922153473 train acc 0.875\n",
    "# epoch 3 batch id 201 loss 0.15983553230762482 train acc 0.9240516169154229\n",
    "# epoch 3 batch id 401 loss 0.14856880903244019 train acc 0.9265118453865336\n",
    "# epoch 3 batch id 601 loss 0.2642267644405365 train acc 0.9280366056572379\n",
    "# epoch 3 batch id 801 loss 0.1951659917831421 train acc 0.93020443196005\n",
    "# epoch 3 batch id 1001 loss 0.26453569531440735 train acc 0.9319586663336663\n",
    "# epoch 3 batch id 1201 loss 0.1282612681388855 train acc 0.9340523522064946\n",
    "# epoch 3 batch id 1401 loss 0.1837957799434662 train acc 0.9360501427551748\n",
    "# epoch 3 batch id 1601 loss 0.14137345552444458 train acc 0.9374414428482198\n",
    "# epoch 3 batch id 1801 loss 0.09849003702402115 train acc 0.9387493059411438\n",
    "# epoch 3 batch id 2001 loss 0.15634101629257202 train acc 0.9401080709645178\n",
    "# epoch 3 batch id 2201 loss 0.1982114464044571 train acc 0.9410495229441163\n",
    "# epoch 3 train acc 0.9420039640216155\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.77it/s]\n",
    "# epoch 3 test acc 0.8969988810741688\n",
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.82it/s]\n",
    "# epoch 4 batch id 1 loss 0.388761043548584 train acc 0.875\n",
    "# epoch 4 batch id 201 loss 0.06205718219280243 train acc 0.9593439054726368\n",
    "# epoch 4 batch id 401 loss 0.06854245811700821 train acc 0.9596711346633416\n",
    "# epoch 4 batch id 601 loss 0.23485814034938812 train acc 0.9600925540765392\n",
    "# epoch 4 batch id 801 loss 0.1342923790216446 train acc 0.9612008426966292\n",
    "# epoch 4 batch id 1001 loss 0.1908232569694519 train acc 0.9621472277722277\n",
    "# epoch 4 batch id 1201 loss 0.11091630905866623 train acc 0.9632597835137385\n",
    "# epoch 4 batch id 1401 loss 0.10650145262479782 train acc 0.9642219842969307\n",
    "# epoch 4 batch id 1601 loss 0.08601253479719162 train acc 0.9649242660836976\n",
    "# epoch 4 batch id 1801 loss 0.057230446487665176 train acc 0.9656093836757357\n",
    "# epoch 4 batch id 2001 loss 0.07411567866802216 train acc 0.9665011244377811\n",
    "# epoch 4 batch id 2201 loss 0.1597125381231308 train acc 0.9671456156292594\n",
    "# epoch 4 train acc 0.9676701151877133\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.77it/s]\n",
    "# epoch 4 test acc 0.8980778452685422\n",
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.81it/s]\n",
    "# epoch 5 batch id 1 loss 0.3727969229221344 train acc 0.890625\n",
    "# epoch 5 batch id 201 loss 0.02794063650071621 train acc 0.9752798507462687\n",
    "# epoch 5 batch id 401 loss 0.024620698764920235 train acc 0.9767378428927681\n",
    "# epoch 5 batch id 601 loss 0.15002880990505219 train acc 0.9765495008319468\n",
    "# epoch 5 batch id 801 loss 0.05448848009109497 train acc 0.9766112671660424\n",
    "# epoch 5 batch id 1001 loss 0.08006531745195389 train acc 0.9770541958041958\n",
    "# epoch 5 batch id 1201 loss 0.04451199620962143 train acc 0.9775317443796836\n",
    "# epoch 5 batch id 1401 loss 0.08561042696237564 train acc 0.9780625446109922\n",
    "# epoch 5 batch id 1601 loss 0.026716381311416626 train acc 0.9783533728919426\n",
    "# epoch 5 batch id 1801 loss 0.02442212402820587 train acc 0.9787357717934481\n",
    "# epoch 5 batch id 2001 loss 0.0197431817650795 train acc 0.9790339205397302\n",
    "# epoch 5 batch id 2201 loss 0.03802771866321564 train acc 0.9792565879145843\n",
    "# epoch 5 train acc 0.9794199729806597\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.78it/s]\n",
    "# epoch 5 test acc 0.8974384590792839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea30fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoBERT",
   "language": "python",
   "name": "kobert-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
