{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8d71b8",
   "metadata": {},
   "source": [
    "### KPC C09K11/06\n",
    "#### CPC 분류 코드\n",
    "* C09K11/06 : 1dot, containing organic luminescent materials (유기 발광성 물질을 함유하는 것)\n",
    "    * C09K11/00 : main group, Luminescent, e.g. electroluminescent, chemiluminescent materials (발광성 물질, 예. 전기 발광성 물질, 화학 발광성 물질)\n",
    "        * C09K : subclass, MATERIALS FOR MISCELLANEOUS APPLICATIONS, NOT PROVIDED FOR ELSEWHERE (그 밖에 분류되지 않는 다수의 응용을 위한 재료)\n",
    "            * C09 : class, DYES; PAINTS; POLISHES; NATURAL RESINS; ADHESIVES; COMPOSITIONS NOT OTHERWISE PROVIDED FOR; APPLICATIONS OF MATERIALS NOT OTHERWISE PROVIDED FOR (염료;  페인트;  광택제;  천연 수지;  접착제;  그 밖에 분류되지 않는 조성물;  그 밖에 분류되지 않는 재료의 응용)\n",
    "                * C : section, CHEMISTRY; METALLURGY (화학;  야금)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2259572",
   "metadata": {},
   "source": [
    "#### KPC 분류 단계\n",
    "* C09K11/06은 CPC이며, KPC는 K1부터 시작됨\n",
    "* decision #01 : CPC C09K11/06 vs 2dot entries(C09K11/06K1, C09K11/06K2)\n",
    "    * 만약 decision #01의 결과가 C09K11/06K2가 아니라면 분류 종료\n",
    "    * 만약 decision #01의 결과가 C09K11/06K2라면 decision #2\n",
    "        * decision #02 : CPC C09K11/06 vs 3dot entries(C09K11/06K21, C09K11/06K22, C09K11/06K23, C09K11/06K24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdec3c",
   "metadata": {},
   "source": [
    "#### C09K11/06 Scheme\n",
    "![](scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a12ac3",
   "metadata": {},
   "source": [
    "#### Decision Process\n",
    "![](C09K11-06.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eddf56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchmetrics import ConfusionMatrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from kobert import get_tokenizer\n",
    "from kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f723d24",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset('kpc_c09k/train_C09K11_220715.txt', field_indices=[0,1], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset('kpc_c09k/test_C09K11_220715.txt', field_indices=[0,1], num_discard_samples=1)\n",
    "# 텍스트 내에 줄바꿈과 탭이 존재하여 오류 발생, 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kpc_c09k/c09k_ind_label.pickle\", \"rb\") as fr:\n",
    "    kpc_index_dict = pickle.load(fr)\n",
    "with open(\"kpc_c09k/c09k_label_ind.pickle\", \"rb\") as fr:\n",
    "    kpc_label_dict = pickle.load(fr)\n",
    "# with open('kpc_c09k/c09k_ind_label.pickle', 'wb') as f:\n",
    "#     pickle.dump(kpc_index_dict, f)\n",
    "# with open('kpc_c09k/c09k_label_ind.pickle', 'wb') as f:\n",
    "#     pickle.dump(kpc_label_dict, f)\n",
    "\n",
    "num_class = len(kpc_index_dict)\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb48a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dataset_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfcc584",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")  ## CPU\n",
    "device = torch.device(\"cuda:0\")  ## GPU\n",
    "load_model_path = 'ksic_model'\n",
    "save_model_path1 = 'kpc_c09k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9671ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size=768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(),\n",
    "                              attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df894f",
   "metadata": {},
   "source": [
    "#### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e114b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KSIC 학습 모델을 불러와서 학습\n",
    "bertmodel = torch.load(os.path.join(load_model_path, 'KSIC_KoBERT.pt'))  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "# bertmodel.load_state_dict(\n",
    "#     torch.load(os.path.join(load_model_path, 'KSIC_model_state_dict.pt')))  # state_dict를 불러 온 후, 모델에 저장\n",
    "_, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertmodel = torch.load(os.path.join(save_model_path1, 'kpc_c09k_KoBERT.pt'))  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "# bertmodel.load_state_dict(\n",
    "#     torch.load(os.path.join(save_model_path1, 'kpc_c09k_KoBERT.pt')))\n",
    "# torch.save(model, os.path.join(save_model_path1, 'kpc_c09k_KoBERT.pt'))  # 전체 모델 저장\n",
    "# torch.save(model.state_dict(), os.path.join(save_model_path1, 'kpc_c09k_model_state_dict.pt'))  # 모델 객체의 state_dict 저장\n",
    "# torch.save({\n",
    "#     'model': model.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "# }, os.path.join(save_model_path1, 'all.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c68120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     bertmodel = torch.load(os.path.join(model_path, 'KSIC_KoBERT.pt'))  # 전체 모델을 통째로 불러옴, 클래스 선언 필수\n",
    "#     bertmodel.load_state_dict(\n",
    "#         torch.load(os.path.join(model_path, 'KSIC_model_state_dict.pt')))  # state_dict를 불러 온 후, 모델에 저장\n",
    "#     print('Using saved model')\n",
    "#     _, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n",
    "#     print('complete loading vocab')\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT 모델을 불러와서 학습\n",
    "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90eda0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac104a4",
   "metadata": {},
   "source": [
    "### GluonNLP Toolkit\n",
    "* GluonNLP Toolkit provides tools for building efficient data pipelines for NLP tasks.\n",
    "* https://nlp.gluon.ai/api/modules/data.html#gluonnlp.data.BERTSentenceTransform\n",
    "* class gluonnlp.data.BERTSPTokenizer(path, vocab, num_best=0, alpha=1.0, lower=True, max_input_chars_per_word=200)[source]¶\n",
    "* https://nlp.gluon.ai/api/modules/data.html#gluonnlp.data.TSVDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba861e5",
   "metadata": {},
   "source": [
    "### GluonNLP BERTSPTokenizer\n",
    "* BERTSPTokenizer depends on the sentencepiece library.\n",
    "* For multi-processing with BERTSPTokenizer, making an extra copy of the BERTSPTokenizer instance is recommended before using it.\n",
    "* https://nlp.gluon.ai/api/data.html?highlight=bertsptokenizer#gluonnlp.data.BERTSPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df56e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01745787",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = dataset_train[2][0][:256]\n",
    "# sample_txt = 'GaN 전력 증폭기를 이용한 전력 오실레이터, 본 발명은 GaN(Gallium Nitride) 소자로 구성되며, 입력 신호의 전력을 증폭시켜 출력하는 GaN 전력 증폭기, 상기 GaN 전력 증폭기의 출력 신호의 일부를 피드백 신호로 제공하는 디렉셔널 커플러, 상기 디렉셔널 커플러에 의해서 제공되는 피드백 신호의 페이저를 가변시키는 페이저 시프터 및 상기 페이저 시프터에 의한 임피던스 부정합을 조정하며, 상기 GaN 전력 증폭기로 상기 피드백 신호를 전달하는 제 1 아이솔레이터를 포함하는 GaN 전력 증폭기를 이용한 전력 오실레이터가 제공된다.'\n",
    "print(tok(sample_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 512\n",
    "batch_size = 8\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 1000\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f38b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_sample_text(ind):\n",
    "    sample_txt = dataset_train[ind][0][:max_len]\n",
    "    print(dataset_train[ind][0])\n",
    "    print(tok(sample_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde47ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst = np.random.randint(0, len(dataset_train), 10)\n",
    "for i in lst:\n",
    "    view_sample_text(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675246bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = ['▁화', '합', '물', '▁및', '▁이를', '▁포함', '하는', '▁유', '기', '▁발', '광', '▁소', '자', '▁하기', '▁화학', '식', '▁1', '로', '▁표시', '되는', '▁화', '합', '물', '▁:', '▁[', '▁화학', '식', '▁1', '▁', ']', '▁상', '기', '▁화학', '식', '▁1', '에', '▁있어서', '▁', ',', '▁L', '1', '▁및', '▁L', '2', '는', '▁서로', '▁같', '거나', '▁상', '이', '하고', '▁', ',', '▁각각', '▁독립', '적으로', '▁직접', '결', '합', '▁', ';', '▁또는', '▁아', '릴', '렌', '기', '이며', '▁', ',', '▁A', 'r', '1', '▁및', '▁A', 'r', '2', '는', '▁서로', '▁같', '거나', '▁상', '이', '하고', '▁', ',', '▁각각', '▁독립', '적으로', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁N', '▁포함', '▁단', '환', '의', '▁헤', '테', '로', '고', '리', '기', '▁', ';', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁N', '포', '함', '▁6', '원', '고', '리', '로', '▁이루', '어', '진', '▁2', '환', '의', '▁헤', '테', '로', '고', '리', '기', '▁', ';', '▁또는', '▁치', '환', '▁또는', '▁', '비치', '환', '된', '▁O', '▁또는', '▁S', '를', '▁포함', '하는', '▁헤', '테', '로', '고', '리', '기', '이다', '▁', '.']\n",
    "len(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13899827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "#         self.labels = [i[label_idx] for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f23e2",
   "metadata": {},
   "source": [
    "#### max_len vs batch size\n",
    "* Text examples should be mostly less than 512 tokens. Longer texts will be cut from the end to fit the sequence length specified in the model block.\n",
    "* https://peltarion.com/knowledge-center/documentation/cheat-sheets/bert---text-classification-/-cheat-sheet\n",
    "* Sequence length\tRecommended max batch size\n",
    "    * 64 - 64, 128 - 32, 256 - 16, 320 - 14, 384 - 12, 512 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_id = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test_id = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aee963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data_train_id.sentences[162])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ab828",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train_id.labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88dc73d",
   "metadata": {},
   "source": [
    "* DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False)\n",
    "* https://pytorch.org/docs/stable/data.html\n",
    "* WARNING\n",
    "    * After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is number of workers * size of parent process). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out issue #13246 for more details on why this occurs and example code for how to workaround these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train_id, batch_size=batch_size, num_workers=5, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test_id, batch_size=batch_size, num_workers=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTClassifier(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  bert,\n",
    "#                  hidden_size = 768,\n",
    "#                  num_classes=2,\n",
    "#                  dr_rate=None,\n",
    "#                  params=None):\n",
    "#         super(BERTClassifier, self).__init__()\n",
    "#         self.bert = bert\n",
    "#         self.dr_rate = dr_rate\n",
    "                 \n",
    "#         self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "#         if dr_rate:\n",
    "#             self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "#     def gen_attention_mask(self, token_ids, valid_length):\n",
    "#         attention_mask = torch.zeros_like(token_ids)\n",
    "#         for i, v in enumerate(valid_length):\n",
    "#             attention_mask[i][:v] = 1\n",
    "#         return attention_mask.float()\n",
    "\n",
    "#     def forward(self, token_ids, valid_length, segment_ids):\n",
    "#         attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "#         _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "#         if self.dr_rate:\n",
    "#             out = self.dropout(pooler)\n",
    "#         else:\n",
    "#             out = pooler\n",
    "#         return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, num_classes=num_class, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895ab36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506ec8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y')) # ' 1:36PM EDT on Oct 18, 2010'\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        val_loss = loss_fn(out, label)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} val_loss {} test acc {}\".format(e+1, val_loss, test_acc / (batch_id+1)))\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y')) # ' 1:36PM EDT on Oct 18, 2010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader2 = torch.utils.data.DataLoader(data_test_id, batch_size=batch_size, num_workers=5)\n",
    "test_acc = 0.0\n",
    "results = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader2), total=len(test_dataloader2)):\n",
    "#     print(batch_id)\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    max_vals, max_indices = torch.max(out, 1)\n",
    "    results.extend(max_indices.tolist())\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "print(\"test acc {}\".format(test_acc / (batch_id+1)))\n",
    "print(results)\n",
    "#     print('label: ', label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3671f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, os.path.join(save_model_path1, 'kpc_c09k_KoBERT.pt'))  # 전체 모델 저장\n",
    "torch.save(model.state_dict(), os.path.join(save_model_path1, 'kpc_c09k_model_state_dict.pt'))  # 모델 객체의 state_dict 저장\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, os.path.join(save_model_path1, 'all.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78983994",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([1, 1, 0, 0])\n",
    "preds = torch.tensor([0, 1, 0, 0])\n",
    "confmat = ConfusionMatrix(num_classes=2)\n",
    "confmat(preds, target)\n",
    "tensor([[2, 0],\n",
    "        [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d528d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(max_indices)\n",
    "# print(max_indices.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe18709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Softmax(dim=1)\n",
    "# exp = m(out)\n",
    "# max_vals, max_indices = torch.max(out, 1)\n",
    "# print(max_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae15239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://dacon.io/en/competitions/official/235747/codeshare/3082?page=1&dtype=recent\n",
    "# def plot_graphs(history, string):\n",
    "#     plt.plot(history.history[string])\n",
    "#     plt.plot(history.history['val_'+string], '')\n",
    "#     plt.xlabel(\"Epochs\")\n",
    "#     plt.ylabel(string)\n",
    "#     plt.legend([string, 'val_'+string])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef28b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 5. 모델 학습 과정 표시하기\n",
    "\n",
    "\n",
    "# fig, loss_ax = plt.subplots()\n",
    "\n",
    "# acc_ax = loss_ax.twinx()\n",
    "\n",
    "# loss_ax.plot(loss.long().to(device), 'y', label='train loss')\n",
    "# # loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "# acc_ax.plot(train_acc.long().to(device), 'b', label='train_acc')\n",
    "# acc_ax.plot(test_acc.long().to(device), 'g', label='test_acc')\n",
    "\n",
    "# loss_ax.set_xlabel('epoch')\n",
    "# loss_ax.set_ylabel('loss')\n",
    "# acc_ax.set_ylabel('accuray')\n",
    "\n",
    "# loss_ax.legend(loc='upper left')\n",
    "# acc_ax.legend(loc='lower left')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d52c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, os.path.join(save_model_path1, 'KSIC_KoBERT.pt'))  # 전체 모델 저장\n",
    "# torch.save(model.state_dict(), os.path.join(save_model_path1, 'KSIC_model_state_dict.pt'))  # 모델 객체의 state_dict 저장\n",
    "# torch.save({\n",
    "#     'model': model.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "# }, os.path.join(save_model_path1, 'all.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(token_ids, valid_length, segment_ids)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ee6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.82it/s]\n",
    "# epoch 1 batch id 1 loss 0.7244999408721924 train acc 0.453125\n",
    "# epoch 1 batch id 201 loss 0.4514557123184204 train acc 0.5766480099502488\n",
    "# epoch 1 batch id 401 loss 0.44362887740135193 train acc 0.6851231296758105\n",
    "# epoch 1 batch id 601 loss 0.5137903094291687 train acc 0.734426996672213\n",
    "# epoch 1 batch id 801 loss 0.43659207224845886 train acc 0.7636352996254682\n",
    "# epoch 1 batch id 1001 loss 0.3066664934158325 train acc 0.7802978271728271\n",
    "# epoch 1 batch id 1201 loss 0.3117211163043976 train acc 0.7930890924229809\n",
    "# epoch 1 batch id 1401 loss 0.30558276176452637 train acc 0.8026075124910778\n",
    "# epoch 1 batch id 1601 loss 0.3304724097251892 train acc 0.8104992973141787\n",
    "# epoch 1 batch id 1801 loss 0.25170841813087463 train acc 0.8164995141588006\n",
    "# epoch 1 batch id 2001 loss 0.2823370695114136 train acc 0.8224481509245377\n",
    "# epoch 1 batch id 2201 loss 0.3238280713558197 train acc 0.8274292935029532\n",
    "# epoch 1 train acc 0.8307713843856656\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 13.16it/s]\n",
    "# epoch 1 test acc 0.8835118286445013\n",
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.81it/s]\n",
    "# epoch 2 batch id 1 loss 0.4955632984638214 train acc 0.828125\n",
    "# epoch 2 batch id 201 loss 0.2225867211818695 train acc 0.8816075870646766\n",
    "# epoch 2 batch id 401 loss 0.33707231283187866 train acc 0.8836502493765586\n",
    "# epoch 2 batch id 601 loss 0.39554905891418457 train acc 0.887115224625624\n",
    "# epoch 2 batch id 801 loss 0.30988579988479614 train acc 0.8883426966292135\n",
    "# epoch 2 batch id 1001 loss 0.28933045268058777 train acc 0.8911713286713286\n",
    "# epoch 2 batch id 1201 loss 0.24474024772644043 train acc 0.8932269983347211\n",
    "# epoch 2 batch id 1401 loss 0.1908964067697525 train acc 0.8957218058529621\n",
    "# epoch 2 batch id 1601 loss 0.23474052548408508 train acc 0.89772993441599\n",
    "# epoch 2 batch id 1801 loss 0.1599130779504776 train acc 0.8997171710161022\n",
    "# epoch 2 batch id 2001 loss 0.20312610268592834 train acc 0.9019865067466267\n",
    "# epoch 2 batch id 2201 loss 0.2386036366224289 train acc 0.9033251930940481\n",
    "# epoch 2 train acc 0.904759047923777\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.77it/s]\n",
    "# epoch 2 test acc 0.8906449808184144\n",
    "# 100%\n",
    "# 2344/2344 [10:51<00:00, 3.81it/s]\n",
    "# epoch 3 batch id 1 loss 0.3390277922153473 train acc 0.875\n",
    "# epoch 3 batch id 201 loss 0.15983553230762482 train acc 0.9240516169154229\n",
    "# epoch 3 batch id 401 loss 0.14856880903244019 train acc 0.9265118453865336\n",
    "# epoch 3 batch id 601 loss 0.2642267644405365 train acc 0.9280366056572379\n",
    "# epoch 3 batch id 801 loss 0.1951659917831421 train acc 0.93020443196005\n",
    "# epoch 3 batch id 1001 loss 0.26453569531440735 train acc 0.9319586663336663\n",
    "# epoch 3 batch id 1201 loss 0.1282612681388855 train acc 0.9340523522064946\n",
    "# epoch 3 batch id 1401 loss 0.1837957799434662 train acc 0.9360501427551748\n",
    "# epoch 3 batch id 1601 loss 0.14137345552444458 train acc 0.9374414428482198\n",
    "# epoch 3 batch id 1801 loss 0.09849003702402115 train acc 0.9387493059411438\n",
    "# epoch 3 batch id 2001 loss 0.15634101629257202 train acc 0.9401080709645178\n",
    "# epoch 3 batch id 2201 loss 0.1982114464044571 train acc 0.9410495229441163\n",
    "# epoch 3 train acc 0.9420039640216155\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.77it/s]\n",
    "# epoch 3 test acc 0.8969988810741688\n",
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.82it/s]\n",
    "# epoch 4 batch id 1 loss 0.388761043548584 train acc 0.875\n",
    "# epoch 4 batch id 201 loss 0.06205718219280243 train acc 0.9593439054726368\n",
    "# epoch 4 batch id 401 loss 0.06854245811700821 train acc 0.9596711346633416\n",
    "# epoch 4 batch id 601 loss 0.23485814034938812 train acc 0.9600925540765392\n",
    "# epoch 4 batch id 801 loss 0.1342923790216446 train acc 0.9612008426966292\n",
    "# epoch 4 batch id 1001 loss 0.1908232569694519 train acc 0.9621472277722277\n",
    "# epoch 4 batch id 1201 loss 0.11091630905866623 train acc 0.9632597835137385\n",
    "# epoch 4 batch id 1401 loss 0.10650145262479782 train acc 0.9642219842969307\n",
    "# epoch 4 batch id 1601 loss 0.08601253479719162 train acc 0.9649242660836976\n",
    "# epoch 4 batch id 1801 loss 0.057230446487665176 train acc 0.9656093836757357\n",
    "# epoch 4 batch id 2001 loss 0.07411567866802216 train acc 0.9665011244377811\n",
    "# epoch 4 batch id 2201 loss 0.1597125381231308 train acc 0.9671456156292594\n",
    "# epoch 4 train acc 0.9676701151877133\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.77it/s]\n",
    "# epoch 4 test acc 0.8980778452685422\n",
    "# 100%\n",
    "# 2344/2344 [10:52<00:00, 3.81it/s]\n",
    "# epoch 5 batch id 1 loss 0.3727969229221344 train acc 0.890625\n",
    "# epoch 5 batch id 201 loss 0.02794063650071621 train acc 0.9752798507462687\n",
    "# epoch 5 batch id 401 loss 0.024620698764920235 train acc 0.9767378428927681\n",
    "# epoch 5 batch id 601 loss 0.15002880990505219 train acc 0.9765495008319468\n",
    "# epoch 5 batch id 801 loss 0.05448848009109497 train acc 0.9766112671660424\n",
    "# epoch 5 batch id 1001 loss 0.08006531745195389 train acc 0.9770541958041958\n",
    "# epoch 5 batch id 1201 loss 0.04451199620962143 train acc 0.9775317443796836\n",
    "# epoch 5 batch id 1401 loss 0.08561042696237564 train acc 0.9780625446109922\n",
    "# epoch 5 batch id 1601 loss 0.026716381311416626 train acc 0.9783533728919426\n",
    "# epoch 5 batch id 1801 loss 0.02442212402820587 train acc 0.9787357717934481\n",
    "# epoch 5 batch id 2001 loss 0.0197431817650795 train acc 0.9790339205397302\n",
    "# epoch 5 batch id 2201 loss 0.03802771866321564 train acc 0.9792565879145843\n",
    "# epoch 5 train acc 0.9794199729806597\n",
    "# 100%\n",
    "# 782/782 [01:06<00:00, 11.78it/s]\n",
    "# epoch 5 test acc 0.8974384590792839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea30fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoBERT",
   "language": "python",
   "name": "kobert-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
