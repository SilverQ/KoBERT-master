{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1eddf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.62 µs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5b7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert import get_tokenizer\n",
    "from kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f24348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3ab2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CPU\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "## GPU\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6be2d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/hdh/PycharmProjects/KoBERT-master/.cache/kobert_v1.zip\n",
      "using cached model. /home/hdh/PycharmProjects/KoBERT-master/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90eda0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1488b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !wget -O /data/ratings_train.txt http://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/datasets/nsmc/ratings_train.txt\n",
    "# # !wget -O /data/ratings_test.txt http://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/datasets/nsmc/ratings_test.txt\n",
    "# # nsmc(Naver Sentiment Movie Corpus) : 네이버 영화 리뷰 감성 분석\n",
    "# !wget -O .cache/ratings_train.txt http://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/datasets/nsmc/ratings_train.txt\n",
    "# !wget -O .cache/ratings_test.txt http://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/datasets/nsmc/ratings_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac104a4",
   "metadata": {},
   "source": [
    "### GluonNLP Toolkit\n",
    "* GluonNLP Toolkit provides tools for building efficient data pipelines for NLP tasks.\n",
    "* https://nlp.gluon.ai/api/modules/data.html#gluonnlp.data.BERTSentenceTransform\n",
    "* class gluonnlp.data.BERTSPTokenizer(path, vocab, num_best=0, alpha=1.0, lower=True, max_input_chars_per_word=200)[source]¶\n",
    "* https://nlp.gluon.ai/api/modules/data.html#gluonnlp.data.TSVDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92d4e36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\".cache/train_H04W4_220511.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\".cache/test_H04W4_220511.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "# 텍스트 내에 줄바꿈과 탭이 존재하여 오류 발생, 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb48a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['본 발명은, 셀룰러 통신 시스템을 이용하여 교통 사고 발생 구간 내의 기지국에 연동되는 휴대 단말에 셀 브로드캐스팅 서비스를 이용하여, 동영상 촬영 요청 메시지를 발신하고, 개별 휴대 단말 사용자로부터 촬영되어 송신된 사고 처리 동영상을 선별하여 저장한 후, 해당 동영상이 링크된 문자 메시지를 사고 발생 구간의 후속 구간에 배치된 기지국(들)에 연동하는 휴대 단말에 발신하여, 원하는 휴대 단말에 사고 처리 동영상을 제공하는, 셀 브로드캐스팅 서비스 및 셀룰러 통신 시스템을 이용한 교통 사고 처리 현황 안내 장치 및 방법을 제공함으로써, 탑승자와의 양방향 정보 제공을 통해 막대한 설치 비용을 수반하는 별도의 동영상 촬상 장치(예컨대, cctv)를 추가하지 않으면서도, 교통 사고 처리 현황에 대한 상세한 동영상을 확보할 수 있으며, 교통 사고 처리 현황에 대해 다양한 각도와 다양한 시야에서 촬영된 동영상을 확보할 수 있다. 이와 같이 확보된 사고 처리 동영상을 사용자가 손쉽게 확인할 수 있도록 함으로써, 교통을 분산시켜 그 소통을 원할하게 할 수 있는 효과가 있다.', '3'], ['차량 통신 서비스를 제공하는 방법 및 장치', '3'], ['생성된 콘텐츠를 BCAST 표준에 맞게 변환하는 단계; 상기 변환된 콘텐츠를 베어러에 맞게 적응시키는 단계; 상기 적응된 콘텐츠를 가입자 단말에 맞게 트랜스코딩하는 단계; 및 상기 트랜스코딩된 콘텐츠를 가입자 단말로 전송하는 단계; 를 포함하는 BCAST 콘텐츠 전송 방법.', '1'], ['적어도 하나의 네트워크 셀(BSC1, BTS1, CELL1; BSC2, BTS2, CELL2, BTS3, CELL3, BTS4, CELL4)을 제어하는 기지국 서브시스템을 포함하는 무선 통신 네트워크(100)에서, 기지국 서브시스템은 무선 블록들을 통하여 상기 셀 내의 이동국들(MS1, MS2, MS3, MS5, MS7)과 통신하고, 상기 기지국 서브시스템에서 데이터 패킷으로 수신된 정보 컨텐츠를 상기 이동국들로 분산시키는 방법에 있어서, 상기 데이터 패킷으로부터 시작하여, 상기 네트워크 셀을 통하여 전송될 무선 블록들(400)을 획득하는 단계; 이동국과 상기 기지국 1.5 서브시스템 사이의 로직 연결을 식별하는 제1 무선 링크 식별자(TFI1)로 상기 무선 블록들을 라벨링(labeling)하는 단계; 상기 네트워크 셀 내의 제1 이동국(MS1, MS3, MS5)으로 상기 제1 무선 링크 식별자를 통신하는 단계; 및 상기 네트워크 셀 내의 적어도 하나의 제2 이동국(MS2, MS7)이 상기 정보 컨텐츠의 수신을 요청할 경우, 상기 제1 무선 링크 식별자를 상기 적어도 하나의 제2 이동국으로 통신하는 단계를 포함한다. 상기 방법은 상기 제1 이동국 및 상기 적어도 하나의 제2 이동국에 상기 무선 블록들에 포함될 각각의 제2 무선 링크 식별자들이 할당되는 단계를 더 포함한다.', '1'], ['1. 발명은 상술한 종래 기술의 문제점을 해결하기 위하여 창안된 것으로서, 본 발명의 목적은 방송망을 통하여 전송된 다양한 데이터를 사용자가 원하는 시간에 시청할 수 있도록 하는 데이터 재생 장치 및 방법을 제공함에 있다.', '1']]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba861e5",
   "metadata": {},
   "source": [
    "### GluonNLP BERTSPTokenizer\n",
    "* BERTSPTokenizer depends on the sentencepiece library.\n",
    "* For multi-processing with BERTSPTokenizer, making an extra copy of the BERTSPTokenizer instance is recommended before using it.\n",
    "* https://nlp.gluon.ai/api/data.html?highlight=bertsptokenizer#gluonnlp.data.BERTSPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df56e69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/hdh/PycharmProjects/KoBERT-master/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd92ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ＷＩＭＡＸ 네트워크 기반의 애드 혹 그룹 방송 서비스 시스템, ＷＩＭＡＸ 네트워크 기반의 애드 혹 그룹 방송 서비스를 위한 애드 혹 그룹 관리자 등록 방법 및 ＷＩＭＡＸ 네트워크 기반의 애드 혹 그룹 방송 서비스 방법\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01745787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁W', 'IM', 'A', 'X', '▁네트워크', '▁기반', '의', '▁애', '드', '▁', '혹', '▁그룹', '▁방송', '▁서비스', '▁시스템', '▁', ',', '▁W', 'IM', 'A', 'X', '▁네트워크', '▁기반', '의', '▁애', '드', '▁', '혹', '▁그룹', '▁방송', '▁서비스를', '▁위한', '▁애', '드', '▁', '혹', '▁그룹', '▁관리', '자', '▁등록', '▁방법', '▁및', '▁W', 'IM', 'A', 'X', '▁네트워크', '▁기반', '의', '▁애', '드', '▁', '혹', '▁그룹', '▁방송', '▁서비스', '▁방법']\n"
     ]
    }
   ],
   "source": [
    "sample_txt = dataset_train[2][0][:256]\n",
    "# sample_txt = 'GaN 전력 증폭기를 이용한 전력 오실레이터, 본 발명은 GaN(Gallium Nitride) 소자로 구성되며, 입력 신호의 전력을 증폭시켜 출력하는 GaN 전력 증폭기, 상기 GaN 전력 증폭기의 출력 신호의 일부를 피드백 신호로 제공하는 디렉셔널 커플러, 상기 디렉셔널 커플러에 의해서 제공되는 피드백 신호의 페이저를 가변시키는 페이저 시프터 및 상기 페이저 시프터에 의한 임피던스 부정합을 조정하며, 상기 GaN 전력 증폭기로 상기 피드백 신호를 전달하는 제 1 아이솔레이터를 포함하는 GaN 전력 증폭기를 이용한 전력 오실레이터가 제공된다.'\n",
    "print(tok(sample_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecde47ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 발명의 목적은 음영 지역에서 EV-DO 망을 이용하여 방송 또는 멀티미디어를 수신하는 방법을 제공하기 위한 것이다.2. 발명의 목적은 음영 지역에서 EV-DO 망을 이용하여 방송 또는 멀티미디어를 수신하는 DMB 단말기를 제공하기 위한 것이다.\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[162][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13899827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "#         self.labels = [i[label_idx] for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90752b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e944083c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bd1e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 512\n",
    "batch_size = 8\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 1000\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f23e2",
   "metadata": {},
   "source": [
    "#### max_len vs batch size\n",
    "* Text examples should be mostly less than 512 tokens. Longer texts will be cut from the end to fit the sequence length specified in the model block.\n",
    "* https://peltarion.com/knowledge-center/documentation/cheat-sheets/bert---text-classification-/-cheat-sheet\n",
    "* Sequence length\tRecommended max batch size\n",
    "    * 64 - 64, 128 - 32, 256 - 16, 320 - 14, 384 - 12, 512 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4dfca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_id = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test_id = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5aee963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   2,  529,  517,   54, 2235, 6210, 2076, 7086, 3606, 6951, 4329,\n",
      "       6903,  645,  357,  524,  644,  329, 1968, 7088, 3726, 7815, 2272,\n",
      "       1862, 2014, 6116, 2872, 6733, 7794, 2270, 7088, 4130, 7789, 3566,\n",
      "        913,  517,   54,  553,  517,   54, 2235, 6210, 2076, 7086, 3606,\n",
      "       6951, 4329, 6903,  645,  357,  524,  644,  329, 1968, 7088, 3726,\n",
      "       7815, 2272, 1862, 2014, 6116, 2872, 6733, 7794,  644,  316,  270,\n",
      "       1589, 6116, 4130, 7789, 3566,  913,  517,   54,    3,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1], dtype=int32), array(75, dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print(data_train_id.sentences[162])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b63ab828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(data_train_id.labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88dc73d",
   "metadata": {},
   "source": [
    "* DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False)\n",
    "* https://pytorch.org/docs/stable/data.html\n",
    "* WARNING\n",
    "    * After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is number of workers * size of parent process). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out issue #13246 for more details on why this occurs and example code for how to workaround these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9df4a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train_id, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test_id, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6c4e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7344847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, num_classes=5, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69bd2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2895ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/KoBERT-master/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2c8e17d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(nn.CrossEntropyLoss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f687765",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8710be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e07f4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d506ec8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9:14AM KST on May 25, 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6fc542d31a4b55825b8434dd4ba7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.5011308193206787 train acc 0.25\n",
      "epoch 1 train acc 0.4640812431842966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc028426490447dba515ff707aad3c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.6844262295081968\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9228d0390041228af53c5a48c13f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.9073958396911621 train acc 0.75\n",
      "epoch 2 train acc 0.6149809160305344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dbc75e27f24237927d59bffeeae9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.701639344262295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023aed9dd69f4a85abc8ace41c80fa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.7084909677505493 train acc 0.625\n",
      "epoch 3 train acc 0.6869547437295529\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92a18659e254bf5ab9c9b55bda74fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.6709016393442623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9420a436091141afbf53f7dcdb5a3ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.2883445918560028 train acc 0.875\n",
      "epoch 4 train acc 0.7333015267175572\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2294f49f5b40218c4d2d7b41c4c6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.7049180327868853\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd35dd296084f1b99be63af7b08450d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.47231897711753845 train acc 0.75\n",
      "epoch 5 train acc 0.7666984732824428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c7b49375d34a5896329b8795474434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.7172131147540983\n",
      " 9:22AM KST on May 25, 2022\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y')) # ' 1:36PM EDT on Oct 18, 2010'\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "print(time.strftime('%l:%M%p %Z on %b %d, %Y')) # ' 1:36PM EDT on Oct 18, 2010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9949130d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7e29697452448fbf2e0a8b443a169d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "test acc 0.625\n",
      "1\n",
      "test acc 0.6875\n",
      "2\n",
      "test acc 0.6666666666666666\n",
      "3\n",
      "test acc 0.6875\n",
      "4\n",
      "test acc 0.725\n",
      "5\n",
      "test acc 0.6875\n",
      "6\n",
      "test acc 0.6607142857142857\n",
      "7\n",
      "test acc 0.671875\n",
      "8\n",
      "test acc 0.6805555555555556\n",
      "9\n",
      "test acc 0.6875\n",
      "10\n",
      "test acc 0.7045454545454546\n",
      "11\n",
      "test acc 0.6979166666666666\n",
      "12\n",
      "test acc 0.6923076923076923\n",
      "13\n",
      "test acc 0.6964285714285714\n",
      "14\n",
      "test acc 0.6916666666666667\n",
      "15\n",
      "test acc 0.6796875\n",
      "16\n",
      "test acc 0.6838235294117647\n",
      "17\n",
      "test acc 0.6875\n",
      "18\n",
      "test acc 0.6907894736842105\n",
      "19\n",
      "test acc 0.70625\n",
      "20\n",
      "test acc 0.7083333333333334\n",
      "21\n",
      "test acc 0.7159090909090909\n",
      "22\n",
      "test acc 0.7119565217391305\n",
      "23\n",
      "test acc 0.71875\n",
      "24\n",
      "test acc 0.71\n",
      "25\n",
      "test acc 0.7115384615384616\n",
      "26\n",
      "test acc 0.7083333333333334\n",
      "27\n",
      "test acc 0.7053571428571429\n",
      "28\n",
      "test acc 0.7155172413793104\n",
      "29\n",
      "test acc 0.7166666666666667\n",
      "30\n",
      "test acc 0.7137096774193549\n",
      "31\n",
      "test acc 0.7109375\n",
      "32\n",
      "test acc 0.7159090909090909\n",
      "33\n",
      "test acc 0.7205882352941176\n",
      "34\n",
      "test acc 0.7178571428571429\n",
      "35\n",
      "test acc 0.7118055555555556\n",
      "36\n",
      "test acc 0.7195945945945946\n",
      "37\n",
      "test acc 0.7236842105263158\n",
      "38\n",
      "test acc 0.717948717948718\n",
      "39\n",
      "test acc 0.715625\n",
      "40\n",
      "test acc 0.7225609756097561\n",
      "41\n",
      "test acc 0.7232142857142857\n",
      "42\n",
      "test acc 0.7267441860465116\n",
      "43\n",
      "test acc 0.7215909090909091\n",
      "44\n",
      "test acc 0.7222222222222222\n",
      "45\n",
      "test acc 0.7255434782608695\n",
      "46\n",
      "test acc 0.7180851063829787\n",
      "47\n",
      "test acc 0.7239583333333334\n",
      "48\n",
      "test acc 0.7219387755102041\n",
      "49\n",
      "test acc 0.72\n",
      "50\n",
      "test acc 0.7205882352941176\n",
      "51\n",
      "test acc 0.71875\n",
      "52\n",
      "test acc 0.7193396226415094\n",
      "53\n",
      "test acc 0.7152777777777778\n",
      "54\n",
      "test acc 0.7113636363636363\n",
      "55\n",
      "test acc 0.7142857142857143\n",
      "56\n",
      "test acc 0.7127192982456141\n",
      "57\n",
      "test acc 0.7112068965517241\n",
      "58\n",
      "test acc 0.7097457627118644\n",
      "59\n",
      "test acc 0.7125\n",
      "60\n",
      "test acc 0.7172131147540983\n",
      "[3, 3, 1, 1, 0, 1, 0, 3, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4, 0, 4, 2, 1, 3, 1, 1, 0, 0, 0, 3, 0, 1, 0, 2, 3, 1, 3, 1, 1, 4, 0, 3, 1, 1, 1, 3, 2, 0, 3, 1, 1, 3, 0, 0, 1, 0, 1, 3, 0, 1, 1, 1, 3, 1, 1, 0, 1, 2, 1, 0, 2, 0, 0, 1, 1, 0, 3, 3, 1, 0, 1, 1, 0, 0, 0, 3, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 3, 0, 0, 0, 1, 1, 3, 2, 0, 1, 1, 1, 1, 1, 3, 2, 0, 1, 1, 1, 0, 0, 1, 3, 1, 3, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 3, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 1, 0, 0, 1, 3, 3, 0, 2, 0, 1, 0, 1, 3, 1, 0, 3, 4, 3, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 3, 1, 1, 0, 1, 1, 0, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 1, 0, 3, 1, 1, 0, 1, 0, 1, 1, 4, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 3, 4, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 2, 3, 3, 4, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 3, 0, 0, 2, 0, 3, 0, 0, 0, 4, 0, 0, 1, 1, 3, 0, 0, 1, 2, 1, 1, 1, 1, 0, 1, 1, 3, 3, 0, 0, 2, 1, 3, 1, 3, 3, 0, 4, 1, 0, 0, 1, 0, 1, 2, 0, 0, 3, 3, 1, 1, 0, 1, 3, 3, 3, 1, 1, 1, 4, 1, 1, 0, 1, 0, 0, 3, 3, 2, 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 3, 3, 1, 3, 1, 0, 0, 1, 3, 1, 2, 2, 2, 2, 0, 4, 1, 0, 0, 1, 0, 0, 1, 3, 1, 0, 3, 1, 0, 1, 0, 1, 1, 2, 0, 1, 1, 0, 1, 0, 1, 3, 3, 2, 2, 1, 0, 4, 1, 1, 3, 0, 3, 0, 0, 1, 0, 3, 0, 1, 0, 3, 1, 3, 0, 0, 3, 0, 1, 0, 0, 1, 1, 1, 1, 3, 3, 4, 0, 0, 1, 0, 3, 1, 0, 2, 3, 4, 0, 3, 1, 1, 2, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 3, 3, 0, 3, 3, 4, 1, 1, 0, 3, 1, 0, 0, 1, 1, 0, 3, 3, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0.0\n",
    "results = []\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "    print(batch_id)\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    max_vals, max_indices = torch.max(out, 1)\n",
    "    results.extend(max_indices.tolist())\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "    print(\"test acc {}\".format(test_acc / (batch_id+1)))\n",
    "print(results)\n",
    "#     print('label: ', label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d528d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 1], device='cuda:0')\n",
      "[0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(max_indices)\n",
    "print(max_indices.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbe18709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "exp = m(out)\n",
    "max_vals, max_indices = torch.max(out, 1)\n",
    "print(max_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ae15239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dacon.io/en/competitions/official/235747/codeshare/3082?page=1&dtype=recent\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fef28b8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m fig, loss_ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m      7\u001b[0m acc_ax \u001b[38;5;241m=\u001b[39m loss_ax\u001b[38;5;241m.\u001b[39mtwinx()\n\u001b[0;32m----> 9\u001b[0m loss_ax\u001b[38;5;241m.\u001b[39mplot(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\u001b[39;00m\n\u001b[1;32m     12\u001b[0m acc_ax\u001b[38;5;241m.\u001b[39mplot(train_acc\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. 모델 학습 과정 표시하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(loss.detach().numpy(), 'y', label='train loss')\n",
    "# loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(train_acc.detach().numpy(), 'b', label='train_acc')\n",
    "acc_ax.plot(test_acc.detach().numpy(), 'g', label='test_acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f06f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(token_ids, valid_length, segment_ids)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be2cbc",
   "metadata": {},
   "source": [
    "100%\n",
    "2344/2344 [10:52<00:00, 3.82it/s]\n",
    "epoch 1 batch id 1 loss 0.7244999408721924 train acc 0.453125\n",
    "epoch 1 batch id 201 loss 0.4514557123184204 train acc 0.5766480099502488\n",
    "epoch 1 batch id 401 loss 0.44362887740135193 train acc 0.6851231296758105\n",
    "epoch 1 batch id 601 loss 0.5137903094291687 train acc 0.734426996672213\n",
    "epoch 1 batch id 801 loss 0.43659207224845886 train acc 0.7636352996254682\n",
    "epoch 1 batch id 1001 loss 0.3066664934158325 train acc 0.7802978271728271\n",
    "epoch 1 batch id 1201 loss 0.3117211163043976 train acc 0.7930890924229809\n",
    "epoch 1 batch id 1401 loss 0.30558276176452637 train acc 0.8026075124910778\n",
    "epoch 1 batch id 1601 loss 0.3304724097251892 train acc 0.8104992973141787\n",
    "epoch 1 batch id 1801 loss 0.25170841813087463 train acc 0.8164995141588006\n",
    "epoch 1 batch id 2001 loss 0.2823370695114136 train acc 0.8224481509245377\n",
    "epoch 1 batch id 2201 loss 0.3238280713558197 train acc 0.8274292935029532\n",
    "epoch 1 train acc 0.8307713843856656\n",
    "100%\n",
    "782/782 [01:06<00:00, 13.16it/s]\n",
    "epoch 1 test acc 0.8835118286445013\n",
    "100%\n",
    "2344/2344 [10:52<00:00, 3.81it/s]\n",
    "epoch 2 batch id 1 loss 0.4955632984638214 train acc 0.828125\n",
    "epoch 2 batch id 201 loss 0.2225867211818695 train acc 0.8816075870646766\n",
    "epoch 2 batch id 401 loss 0.33707231283187866 train acc 0.8836502493765586\n",
    "epoch 2 batch id 601 loss 0.39554905891418457 train acc 0.887115224625624\n",
    "epoch 2 batch id 801 loss 0.30988579988479614 train acc 0.8883426966292135\n",
    "epoch 2 batch id 1001 loss 0.28933045268058777 train acc 0.8911713286713286\n",
    "epoch 2 batch id 1201 loss 0.24474024772644043 train acc 0.8932269983347211\n",
    "epoch 2 batch id 1401 loss 0.1908964067697525 train acc 0.8957218058529621\n",
    "epoch 2 batch id 1601 loss 0.23474052548408508 train acc 0.89772993441599\n",
    "epoch 2 batch id 1801 loss 0.1599130779504776 train acc 0.8997171710161022\n",
    "epoch 2 batch id 2001 loss 0.20312610268592834 train acc 0.9019865067466267\n",
    "epoch 2 batch id 2201 loss 0.2386036366224289 train acc 0.9033251930940481\n",
    "epoch 2 train acc 0.904759047923777\n",
    "100%\n",
    "782/782 [01:06<00:00, 11.77it/s]\n",
    "epoch 2 test acc 0.8906449808184144\n",
    "100%\n",
    "2344/2344 [10:51<00:00, 3.81it/s]\n",
    "epoch 3 batch id 1 loss 0.3390277922153473 train acc 0.875\n",
    "epoch 3 batch id 201 loss 0.15983553230762482 train acc 0.9240516169154229\n",
    "epoch 3 batch id 401 loss 0.14856880903244019 train acc 0.9265118453865336\n",
    "epoch 3 batch id 601 loss 0.2642267644405365 train acc 0.9280366056572379\n",
    "epoch 3 batch id 801 loss 0.1951659917831421 train acc 0.93020443196005\n",
    "epoch 3 batch id 1001 loss 0.26453569531440735 train acc 0.9319586663336663\n",
    "epoch 3 batch id 1201 loss 0.1282612681388855 train acc 0.9340523522064946\n",
    "epoch 3 batch id 1401 loss 0.1837957799434662 train acc 0.9360501427551748\n",
    "epoch 3 batch id 1601 loss 0.14137345552444458 train acc 0.9374414428482198\n",
    "epoch 3 batch id 1801 loss 0.09849003702402115 train acc 0.9387493059411438\n",
    "epoch 3 batch id 2001 loss 0.15634101629257202 train acc 0.9401080709645178\n",
    "epoch 3 batch id 2201 loss 0.1982114464044571 train acc 0.9410495229441163\n",
    "epoch 3 train acc 0.9420039640216155\n",
    "100%\n",
    "782/782 [01:06<00:00, 11.77it/s]\n",
    "epoch 3 test acc 0.8969988810741688\n",
    "100%\n",
    "2344/2344 [10:52<00:00, 3.82it/s]\n",
    "epoch 4 batch id 1 loss 0.388761043548584 train acc 0.875\n",
    "epoch 4 batch id 201 loss 0.06205718219280243 train acc 0.9593439054726368\n",
    "epoch 4 batch id 401 loss 0.06854245811700821 train acc 0.9596711346633416\n",
    "epoch 4 batch id 601 loss 0.23485814034938812 train acc 0.9600925540765392\n",
    "epoch 4 batch id 801 loss 0.1342923790216446 train acc 0.9612008426966292\n",
    "epoch 4 batch id 1001 loss 0.1908232569694519 train acc 0.9621472277722277\n",
    "epoch 4 batch id 1201 loss 0.11091630905866623 train acc 0.9632597835137385\n",
    "epoch 4 batch id 1401 loss 0.10650145262479782 train acc 0.9642219842969307\n",
    "epoch 4 batch id 1601 loss 0.08601253479719162 train acc 0.9649242660836976\n",
    "epoch 4 batch id 1801 loss 0.057230446487665176 train acc 0.9656093836757357\n",
    "epoch 4 batch id 2001 loss 0.07411567866802216 train acc 0.9665011244377811\n",
    "epoch 4 batch id 2201 loss 0.1597125381231308 train acc 0.9671456156292594\n",
    "epoch 4 train acc 0.9676701151877133\n",
    "100%\n",
    "782/782 [01:06<00:00, 11.77it/s]\n",
    "epoch 4 test acc 0.8980778452685422\n",
    "100%\n",
    "2344/2344 [10:52<00:00, 3.81it/s]\n",
    "epoch 5 batch id 1 loss 0.3727969229221344 train acc 0.890625\n",
    "epoch 5 batch id 201 loss 0.02794063650071621 train acc 0.9752798507462687\n",
    "epoch 5 batch id 401 loss 0.024620698764920235 train acc 0.9767378428927681\n",
    "epoch 5 batch id 601 loss 0.15002880990505219 train acc 0.9765495008319468\n",
    "epoch 5 batch id 801 loss 0.05448848009109497 train acc 0.9766112671660424\n",
    "epoch 5 batch id 1001 loss 0.08006531745195389 train acc 0.9770541958041958\n",
    "epoch 5 batch id 1201 loss 0.04451199620962143 train acc 0.9775317443796836\n",
    "epoch 5 batch id 1401 loss 0.08561042696237564 train acc 0.9780625446109922\n",
    "epoch 5 batch id 1601 loss 0.026716381311416626 train acc 0.9783533728919426\n",
    "epoch 5 batch id 1801 loss 0.02442212402820587 train acc 0.9787357717934481\n",
    "epoch 5 batch id 2001 loss 0.0197431817650795 train acc 0.9790339205397302\n",
    "epoch 5 batch id 2201 loss 0.03802771866321564 train acc 0.9792565879145843\n",
    "epoch 5 train acc 0.9794199729806597\n",
    "100%\n",
    "782/782 [01:06<00:00, 11.78it/s]\n",
    "epoch 5 test acc 0.8974384590792839"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KoBERT",
   "language": "python",
   "name": "kobert-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
